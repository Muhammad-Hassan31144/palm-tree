"""
Network Analysis Module (network_analysis.py)

Purpose:
This module analyzes network traffic captured during malware execution to identify
malicious network behaviors, command & control communications, data exfiltration,
and network-based indicators of compromise.

Context in Shikra:
- Input: PCAP files from core/modules/network/capture.py
- Processing: Traffic analysis, protocol inspection, pattern detection
- Output: Network IOCs, C2 analysis, and communication patterns for reporting

Key Functionalities:
The NetworkAnalyzer class processes network captures to identify:
- Command and Control (C2) server communications
- Domain Generation Algorithm (DGA) patterns
- Data exfiltration over network protocols
- Malicious DNS queries and responses
- Encrypted traffic analysis (JA3/JARM fingerprinting)
- Protocol anomalies and covert channels

Integration Points:
- Processes PCAP files generated by core/modules/network/capture.py
- May integrate with threat intelligence feeds for IP/domain reputation
- Outputs network IOCs to reporting/modules/reporting/report_generator.py
"""

import logging
import os
import re
import json
import math
import hashlib
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
import ipaddress
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from urllib.parse import urlparse
import statistics

# Try to import optional dependencies for PCAP analysis
try:
    import dpkt
    import dpkt.dns
    import dpkt.http
    DPKT_AVAILABLE = True
except ImportError:
    DPKT_AVAILABLE = False
    dpkt = None

try:
    import pyshark
    PYSHARK_AVAILABLE = True
except ImportError:
    PYSHARK_AVAILABLE = False
    pyshark = None

try:
    import scapy.all as scapy
    from scapy.layers.dns import DNS, DNSQR, DNSRR
    from scapy.layers.http import HTTP, HTTPRequest, HTTPResponse
    # Scapy's TLS layer might be under scapy.layers.tls.all or just scapy.layers.tls
    # Trying a common import path
    try:
        from scapy.layers.tls.all import TLS
    except ImportError:
        try:
            from scapy.layers.tls import TLS
        except ImportError:
            TLS = None # TLS layer not found

    SCAPY_AVAILABLE = True
    if TLS is None and SCAPY_AVAILABLE:
        logger.warning("Scapy is available, but its TLS layer could not be imported. TLS analysis with Scapy might be limited.")

except ImportError:
    SCAPY_AVAILABLE = False
    scapy = None
    TLS = None


# Configure logging for this module
logger = logging.getLogger(__name__)

class NetworkAnalyzer:
    """
    Analyzes network traffic captures to identify malicious patterns,
    C2 communications, and network-based indicators of compromise.
    
    This class serves as the primary network analysis engine in Shikra,
    processing PCAP files and applying various detection techniques to
    identify network threats and extract actionable intelligence.
    """
    
    def __init__(self, threat_intel_sources: Optional[List[str]] = None):
        """
        Initialize the network analyzer with optional threat intelligence sources.
        
        Args:
            threat_intel_sources (Optional[List[str]]): List of threat intelligence
                                                        sources for IP/domain reputation
        """
        self.threat_intel_sources = threat_intel_sources or []
        self.known_malicious_ips = set()
        self.known_malicious_domains = set()
        self.analysis_results = {}
        self.temp_dir = None
        
        # Initialize detection patterns and thresholds
        self._init_detection_patterns()
        
        if threat_intel_sources:
            self._load_threat_intelligence()
            
        logger.info("NetworkAnalyzer initialized")
    
    def _init_detection_patterns(self):
        """Initialize detection patterns and thresholds"""
        
        # Suspicious TLDs commonly used by malware
        self.suspicious_tlds = {
            '.tk', '.ml', '.ga', '.cf', '.top', '.click', '.download',
            '.work', '.online', '.site', '.website', '.space', '.tech',
            '.club', '.win', '.bid', '.loan', '.date', '.review'
        }
        
        # Common malware user agents
        self.malicious_user_agents = [
            'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)',
            'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)',
            'curl/', 'wget/', 'python-requests/', 'python-urllib/',
            'WinHTTP', 'Microsoft-CryptoAPI', 'Microsoft BITS/',
            'Mozila/', 'Mozzila/', 'Chrome/'  # Typos in legitimate UA strings
        ]
        
        # Suspicious HTTP paths/URIs commonly used by malware
        self.suspicious_uri_patterns = [
            r'/gate\.php', r'/panel/', r'/admin\.php', r'/config\.php',
            r'/upload\.php', r'/download\.php', r'/cmd\.php', r'/shell\.php',
            r'/backdoor\.php', r'/webshell\.php', r'\.asp\?', r'\.jsp\?',
            r'/api/v\d+/', r'/rest/', r'/rpc/', r'/xmlrpc\.php'
        ]
        
        # Common C2 beaconing intervals (in seconds)
        self.beacon_intervals = [30, 60, 120, 300, 600, 900, 1800, 3600]
        
        # DGA detection parameters
        self.dga_params = {
            'min_entropy': 3.5,
            'max_vowel_ratio': 0.4, # Max vowel ratio for a non-dictionary like domain
            'min_consonant_clusters': 2,
            'min_domain_length': 8, # Min length of the first part of the domain (subdomain)
            'max_dictionary_ratio': 0.3 # Max ratio of dictionary words found
        }
        
        # Protocol anomaly thresholds
        self.anomaly_thresholds = {
            'dns_query_rate': 100,  # queries per minute
            'http_request_rate': 500,  # requests per minute
            'unique_domains_rate': 50,  # unique domains per minute
            'data_transfer_threshold': 100 * 1024 * 1024,  # 100MB
            'connection_duration_max': 3600  # 1 hour
        }
        
        # Known legitimate domains to filter out
        self.legitimate_domains = {
            'google.com', 'microsoft.com', 'apple.com', 'adobe.com',
            'mozilla.org', 'github.com', 'stackoverflow.com', 'wikipedia.org',
            'cloudflare.com', 'amazonaws.com', 'azure.com', 'dropbox.com'
        }
        
        # Common malware port numbers
        self.suspicious_ports = {
            1337, 4444, 5555, 6666, 7777, 8080, 8443, 9999,
            31337, 54321, 65534, 1234, 1433, 3389, 445, 135
        }
    
    def _load_threat_intelligence(self):
        """
        Load threat intelligence data from configured sources.
        Populates known malicious IPs and domains for reputation checking.
        """
        logger.info("Loading threat intelligence data...")
        
        # Mock implementation - in a real system, this would connect to threat intel APIs
        # Example sources: VirusTotal, AlienVault OTX, Malware Domain List, etc.
        
        # Sample malicious indicators (normally loaded from external sources)
        sample_malicious_ips = [
            '198.51.100.1', '203.0.113.1', '192.0.2.1',  # RFC test IPs
            '185.220.100.240', '185.220.101.1'  # Example malicious IPs
        ]
        
        sample_malicious_domains = [
            'evil-domain.com', 'malware-c2.net', 'badactor.org',
            'phishing-site.tk', 'trojan-downloader.ml'
        ]
        
        self.known_malicious_ips.update(sample_malicious_ips)
        self.known_malicious_domains.update(sample_malicious_domains)
        
        logger.info(f"Loaded {len(self.known_malicious_ips)} malicious IPs and {len(self.known_malicious_domains)} malicious domains")
    
    def analyze_pcap_file(self, pcap_path: Path) -> Dict[str, Any]:
        """
        Main analysis function that processes a PCAP file and returns comprehensive results.
        
        Args:
            pcap_path (Path): Path to the PCAP file to analyze
            
        Returns:
            Dict[str, Any]: Complete network analysis results including traffic summary,
                            detected threats, IOCs, and communication patterns
        """
        logger.info(f"Starting network analysis of PCAP: {pcap_path}")
        
        if not pcap_path.exists():
            logger.error(f"PCAP file not found: {pcap_path}")
            raise FileNotFoundError(f"PCAP file not found: {pcap_path}")
        
        # Create temporary directory for analysis artifacts
        try:
            self.temp_dir = Path(tempfile.mkdtemp(prefix='shikra_network_'))
        except Exception as e:
            logger.error(f"Failed to create temporary directory: {e}")
            self.temp_dir = None # Ensure temp_dir is None if creation fails

        # Initialize results structure
        analysis_results = {
            'pcap_info': {
                'file_path': str(pcap_path),
                'file_size': pcap_path.stat().st_size if pcap_path.exists() else 0,
                'analysis_timestamp': datetime.now().isoformat()
            },
            'traffic_summary': {},
            'dns_analysis': {},
            'http_analysis': {},
            'c2_communications': [],
            'dga_domains': [],
            'data_exfiltration': [],
            'encrypted_traffic': {},
            'protocol_anomalies': [],
            'network_iocs': {},
            'threat_indicators': [],
            'correlation_data': {},
            'errors': []
        }
            
        try:
            # Generate traffic summary
            logger.info("Generating traffic summary...")
            analysis_results['traffic_summary'] = self.generate_traffic_summary(pcap_path)
            
            # Extract and analyze DNS queries
            logger.info("Analyzing DNS traffic...")
            dns_queries = self.extract_dns_queries(pcap_path)
            analysis_results['dns_analysis'] = {
                'queries': dns_queries,
                'total_queries': len(dns_queries),
                'unique_domains': len(set(q.get('domain', '') for q in dns_queries if q.get('domain'))),
                'suspicious_queries': [q for q in dns_queries if q.get('suspicious', False)]
            }
            
            # Detect DGA domains
            logger.info("Detecting DGA domains...")
            domain_list = [q.get('domain', '') for q in dns_queries if q.get('domain')]
            analysis_results['dga_domains'] = self.detect_dga_domains(domain_list)
            
            # Analyze HTTP traffic
            logger.info("Analyzing HTTP traffic...")
            analysis_results['http_analysis'] = self.analyze_http_traffic(pcap_path)
            
            # Detect C2 communications
            logger.info("Detecting C2 communications...")
            analysis_results['c2_communications'] = self.detect_c2_communications(pcap_path)
            
            # Detect data exfiltration
            logger.info("Detecting data exfiltration...")
            analysis_results['data_exfiltration'] = self.detect_data_exfiltration(pcap_path)
            
            # Analyze encrypted traffic
            logger.info("Analyzing encrypted traffic...")
            analysis_results['encrypted_traffic'] = self.analyze_encrypted_traffic(pcap_path)
            
            # Detect protocol anomalies
            logger.info("Detecting protocol anomalies...")
            analysis_results['protocol_anomalies'] = self.detect_protocol_anomalies(pcap_path)
            
            # Extract network IOCs
            logger.info("Extracting network IOCs...")
            analysis_results['network_iocs'] = self.extract_network_iocs(analysis_results)
            
            # Generate threat indicators summary
            analysis_results['threat_indicators'] = self._generate_threat_indicators(analysis_results)
            
            self.analysis_results = analysis_results
            logger.info("Network analysis completed successfully")
            
            return analysis_results
            
        except Exception as e:
            logger.error(f"Network analysis failed: {e}", exc_info=True)
            analysis_results['errors'].append(f"Overall analysis error: {str(e)}")
            # Depending on desired behavior, you might re-raise or return partial results
            # For now, returning partial results with error noted.
            return analysis_results # Return partial results
        finally:
            # Cleanup temporary files
            if self.temp_dir and self.temp_dir.exists():
                import shutil
                try:
                    shutil.rmtree(self.temp_dir)
                    logger.info(f"Successfully removed temporary directory: {self.temp_dir}")
                except Exception as e:
                    logger.error(f"Failed to remove temporary directory {self.temp_dir}: {e}")

    def extract_dns_queries(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """
        Extract and analyze DNS queries from network traffic.
        
        Processes DNS traffic to identify:
        - Domain resolution requests
        - DNS tunneling attempts
        - Fast-flux DNS patterns
        - DNS over HTTPS/TLS usage
        - Suspicious TLD usage
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            List[Dict[str, Any]]: DNS queries with analysis results and reputation data
        """
        dns_queries = []
        
        try:
            if SCAPY_AVAILABLE:
                dns_queries = self._extract_dns_with_scapy(pcap_path)
            elif DPKT_AVAILABLE:
                dns_queries = self._extract_dns_with_dpkt(pcap_path)
            elif PYSHARK_AVAILABLE:
                dns_queries = self._extract_dns_with_pyshark(pcap_path)
            else:
                # Fallback to tshark if available
                dns_queries = self._extract_dns_with_tshark(pcap_path)
            
            # Analyze each DNS query for suspicious characteristics
            for query in dns_queries:
                self._analyze_dns_query(query)
            
            logger.info(f"Extracted and analyzed {len(dns_queries)} DNS queries")
            return dns_queries
            
        except Exception as e:
            logger.error(f"DNS extraction failed: {e}", exc_info=True)
            return []
    
    def detect_c2_communications(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """
        Identify potential Command and Control server communications.
        
        Analyzes traffic patterns to detect:
        - Beaconing behavior (regular callback intervals)
        - Suspicious user agents and HTTP headers
        - Encrypted C2 channels
        - Protocol tunneling
        - Heartbeat communications
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            List[Dict[str, Any]]: Potential C2 communications with confidence scores
        """
        c2_communications = []
        
        try:
            # Extract HTTP connections for C2 analysis
            http_connections = self._extract_http_connections(pcap_path)
            
            # Group connections by destination IP/domain
            connections_by_dest = defaultdict(list)
            for conn in http_connections:
                dest_key = conn.get('dest_ip', '') or conn.get('host', '')
                if dest_key:
                    connections_by_dest[dest_key].append(conn)
            
            # Analyze each destination for C2 patterns
            for dest, connections in connections_by_dest.items():
                if connections: # Ensure there are connections to analyze
                    c2_indicators = self._analyze_c2_patterns(dest, connections)
                    if c2_indicators['confidence_score'] > 0.3:  # Threshold for C2 detection
                        c2_communications.append(c2_indicators)
            
            # Detect beaconing behavior (generic, non-HTTP specific if implemented)
            beaconing_comms = self._detect_beaconing_behavior(pcap_path)
            c2_communications.extend(beaconing_comms)
            
            # Sort by confidence score
            c2_communications.sort(key=lambda x: x.get('confidence_score', 0), reverse=True)
            
            logger.info(f"Detected {len(c2_communications)} potential C2 communications")
            return c2_communications
            
        except Exception as e:
            logger.error(f"C2 detection failed: {e}", exc_info=True)
            return []
    
    def detect_dga_domains(self, domain_list: List[str]) -> List[Dict[str, Any]]:
        """
        Detect Domain Generation Algorithm (DGA) generated domains.
        
        Analyzes domain names for characteristics of DGA:
        - High entropy/randomness
        - Unusual character distributions
        - Dictionary word combinations
        - Length patterns
        - Registration patterns
        
        Args:
            domain_list (List[str]): List of queried domain names (can have duplicates)
            
        Returns:
            List[Dict[str, Any]]: Domains suspected of being DGA-generated with scores
        """
        dga_domains = []
        
        try:
            # Use set to process unique domains
            for domain in set(d for d in domain_list if d): # Filter out empty strings
                if domain in self.legitimate_domains:
                    continue
                
                dga_score = self._calculate_dga_score(domain)
                if dga_score > 0.5:  # Threshold for DGA detection
                    characteristics = self._analyze_domain_characteristics(domain)
                    dga_domains.append({
                        'domain': domain,
                        'dga_score': dga_score,
                        'characteristics': characteristics,
                        'tld': characteristics.get('tld', ''),
                        'suspicious_tld': any(domain.endswith(tld) for tld in self.suspicious_tlds),
                        'entropy': characteristics.get('entropy', 0.0), 
                        'length': characteristics.get('length', 0)
                    })
            
            # Sort by DGA score
            dga_domains.sort(key=lambda x: x['dga_score'], reverse=True)
            
            logger.info(f"Detected {len(dga_domains)} potential DGA domains")
            return dga_domains
            
        except Exception as e:
            logger.error(f"DGA detection failed: {e}", exc_info=True)
            return []

    def analyze_http_traffic(self, pcap_path: Path) -> Dict[str, Any]:
        """
        Analyze HTTP/HTTPS traffic for malicious patterns.
        
        Examines HTTP communications for:
        - Suspicious URLs and paths
        - Malicious payloads in requests/responses
        - Exfiltration over HTTP
        - C2 over HTTP protocols
        - Certificate analysis for HTTPS (basic, via TLS analysis if applicable)
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            Dict[str, Any]: HTTP traffic analysis results with suspicious activities
        """
        http_analysis = {
            'total_requests': 0,
            'unique_hosts': 0,
            'suspicious_requests': [],
            'user_agents': Counter(),
            'status_codes': Counter(), # Note: This requires parsing HTTP responses, not just requests
            'methods': Counter(),
            'content_types': Counter(), # From request headers
            'suspicious_urls': [],
            'large_transfers': [],
            'post_requests': [],
            'error_summary': []
        }
        
        try:
            http_requests = self._extract_http_requests(pcap_path) # This extracts requests
            http_analysis['total_requests'] = len(http_requests)
            
            hosts = set()
            for request in http_requests:
                # Basic statistics from requests
                http_analysis['methods'][request.get('method', 'UNKNOWN')] += 1
                # Status codes would require response parsing, which is not explicitly done here.
                # For now, we'll rely on request data.
                http_analysis['user_agents'][request.get('user_agent', 'Unknown')] += 1
                http_analysis['content_types'][request.get('content_type', 'Unknown')] += 1
                
                host = request.get('host', '')
                if host:
                    hosts.add(host)
                
                # Analyze for suspicious characteristics
                if self._is_suspicious_http_request(request):
                    http_analysis['suspicious_requests'].append(request)
                
                # Check for suspicious URLs
                # Construct full URL if possible, otherwise use URI
                full_url = ""
                uri = request.get('uri', '')
                if host and uri:
                    scheme = "http" # Assume http, or detect from port (e.g. 443 for https)
                    if request.get('dst_port') == 443: # A simple heuristic
                        scheme = "https"
                    full_url = f"{scheme}://{host}{uri}"
                elif uri:
                    full_url = uri # Fallback to URI if host is missing

                if full_url and self._is_suspicious_url(full_url):
                    http_analysis['suspicious_urls'].append({
                        'url': full_url,
                        'host': host,
                        'method': request.get('method'),
                        'timestamp': request.get('timestamp')
                    })
                
                # Track large transfers (based on request Content-Length)
                content_length = request.get('content_length', 0)
                if content_length > 10 * 1024 * 1024:  # 10MB threshold for uploads
                    http_analysis['large_transfers'].append({
                        'url': full_url,
                        'host': host,
                        'size': content_length,
                        'direction': 'upload' # Only know upload size from request
                    })
                
                # Track POST requests (potential data exfiltration)
                if request.get('method') == 'POST':
                    http_analysis['post_requests'].append({
                        'url': full_url,
                        'host': host,
                        'content_length': content_length,
                        'content_type': request.get('content_type'),
                        'timestamp': request.get('timestamp')
                    })
            
            http_analysis['unique_hosts'] = len(hosts)
            
            logger.info(f"Analyzed {len(http_requests)} HTTP requests to {len(hosts)} unique hosts")
            return http_analysis
            
        except Exception as e:
            logger.error(f"HTTP analysis failed: {e}", exc_info=True)
            http_analysis['error_summary'].append(str(e))
            return http_analysis

    def detect_data_exfiltration(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """
        Identify potential data exfiltration over network protocols.
        
        Analyzes traffic for exfiltration indicators:
        - Large outbound data transfers (currently focused on HTTP POST and DNS Tunneling)
        - Unusual protocol usage (DNS tunneling, ICMP tunneling)
        - Data encoding/encryption patterns (DNS tunneling base64 check)
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            List[Dict[str, Any]]: Potential exfiltration activities with volume and methods
        """
        exfiltration_activities = []
        
        try:
            # Analyze large data transfers (currently only from HTTP POST in analyze_http_traffic)
            # This section could be expanded to analyze raw TCP/UDP flows for large outbound data
            # For now, we rely on specific protocol exfiltration detection.
            # large_transfers_generic = self._detect_large_transfers(pcap_path) # Placeholder
            # for transfer in large_transfers_generic:
            #     if transfer.get('bytes_out', 0) > self.anomaly_thresholds['data_transfer_threshold']:
            #         exfiltration_activities.append({
            #             'type': 'large_data_transfer_generic',
            #             'destination': transfer.get('dest_ip'),
            #             'port': transfer.get('dest_port'),
            #             'protocol': transfer.get('protocol'),
            #             'bytes_transferred': transfer.get('bytes_out'),
            #             'duration': transfer.get('duration'),
            #             'confidence': 0.7, # Confidence might vary based on context
            #             'details': transfer
            #         })
            
            # Detect DNS tunneling
            dns_tunneling = self._detect_dns_tunneling(pcap_path) # This method was fixed
            exfiltration_activities.extend(dns_tunneling) # _detect_dns_tunneling now returns list of dicts
            
            # Detect HTTP-based exfiltration (from analyze_http_traffic post_requests and large_transfers)
            http_exfil = self._detect_http_exfiltration(pcap_path) # This method was a placeholder, now uses http_analysis
            exfiltration_activities.extend(http_exfil)
            
            # Detect ICMP tunneling
            icmp_tunneling = self._detect_icmp_tunneling(pcap_path)
            exfiltration_activities.extend(icmp_tunneling)
            
            # Sort by confidence
            exfiltration_activities.sort(key=lambda x: x.get('confidence', 0), reverse=True)
            
            logger.info(f"Detected {len(exfiltration_activities)} potential data exfiltration activities")
            return exfiltration_activities
            
        except Exception as e:
            logger.error(f"Data exfiltration detection failed: {e}", exc_info=True)
            return []

    def analyze_encrypted_traffic(self, pcap_path: Path) -> Dict[str, Any]:
        """
        Analyze encrypted traffic patterns without decryption.
        
        Examines encrypted communications for:
        - TLS certificate analysis (basic)
        - JA3/JARM fingerprinting (simplified JA3)
        - Traffic flow analysis (placeholder)
        - Timing correlations (placeholder)
        - Metadata examination
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            Dict[str, Any]: Encrypted traffic analysis with fingerprints and patterns
        """
        encrypted_analysis = {
            'tls_connections': [],
            'certificates': [], # List of extracted certificate details
            'ja3_fingerprints': defaultdict(list), # JA3 hash -> list of destination IPs
            'cipher_suites': Counter(),
            'tls_versions': Counter(),
            'suspicious_certificates': [], # List of suspicious cert events
            'encrypted_protocols': Counter(), # e.g. TLS, SSH (if detected)
            'flow_analysis': {} # Placeholder for more detailed flow metrics
        }
        
        try:
            # Extract TLS connections
            tls_connections = self._extract_tls_connections(pcap_path)
            encrypted_analysis['tls_connections'] = tls_connections
            
            if tls_connections:
                 encrypted_analysis['encrypted_protocols']['TLS'] = len(tls_connections)

            for conn in tls_connections:
                # Analyze TLS versions and cipher suites
                tls_version = conn.get('tls_version', 'Unknown')
                cipher_suite = conn.get('cipher_suite', 'Unknown')
                
                if tls_version != 'Unknown':
                    encrypted_analysis['tls_versions'][tls_version] += 1
                if cipher_suite != 'Unknown':
                    encrypted_analysis['cipher_suites'][cipher_suite] += 1
                
                # Analyze certificates
                cert_info = conn.get('certificate', {}) # Assume _extract_tls_connections populates this
                if cert_info:
                    encrypted_analysis['certificates'].append(cert_info)
                    
                    # Check for suspicious certificate characteristics
                    if self._is_suspicious_certificate(cert_info):
                        encrypted_analysis['suspicious_certificates'].append({
                            'destination_ip': conn.get('dest_ip'),
                            'server_name_indication': conn.get('server_name', ''), # SNI if available
                            'certificate': cert_info,
                            'reasons': self._get_cert_suspicion_reasons(cert_info)
                        })
                
                # Generate JA3 fingerprint if possible
                ja3_hash = self._generate_ja3_fingerprint(conn) # conn should contain necessary fields
                if ja3_hash:
                    # Store unique destination IPs for each JA3
                    if conn.get('dest_ip') not in encrypted_analysis['ja3_fingerprints'][ja3_hash]:
                         encrypted_analysis['ja3_fingerprints'][ja3_hash].append(conn.get('dest_ip'))
            
            # Analyze encrypted traffic flows (placeholder)
            # encrypted_analysis['flow_analysis'] = self._analyze_encrypted_flows(pcap_path)
            
            logger.info(f"Analyzed {len(tls_connections)} TLS connections")
            # Convert defaultdict to dict for cleaner output if needed
            encrypted_analysis['ja3_fingerprints'] = dict(encrypted_analysis['ja3_fingerprints'])
            return encrypted_analysis
            
        except Exception as e:
            logger.error(f"Encrypted traffic analysis failed: {e}", exc_info=True)
            encrypted_analysis['ja3_fingerprints'] = dict(encrypted_analysis['ja3_fingerprints']) # ensure it's a dict
            return encrypted_analysis

    def detect_protocol_anomalies(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """
        Identify protocol anomalies and potential covert channels.
        
        Detects unusual protocol usage:
        - Protocol tunneling (DNS, ICMP already covered in data_exfiltration)
        - Covert channels in ICMP, DNS, etc.
        - Non-standard port usage
        - Protocol violations
        - Steganographic communications (very advanced, likely placeholder)
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            List[Dict[str, Any]]: Detected protocol anomalies with severity ratings
        """
        anomalies = []
        
        try:
            # Detect port/protocol mismatches
            port_anomalies = self._detect_port_anomalies(pcap_path)
            anomalies.extend(port_anomalies)
            
            # Detect protocol tunneling (other than DNS/ICMP already handled)
            # tunneling_anomalies = self._detect_protocol_tunneling(pcap_path) # Placeholder
            # anomalies.extend(tunneling_anomalies)
            
            # Detect unusual packet sizes
            size_anomalies = self._detect_packet_size_anomalies(pcap_path)
            anomalies.extend(size_anomalies)
            
            # Detect timing anomalies (e.g. fixed inter-packet delays not typical for protocol)
            # timing_anomalies = self._detect_timing_anomalies(pcap_path) # Placeholder
            # anomalies.extend(timing_anomalies)
            
            # Detect covert channels (advanced, e.g. data in unused header fields)
            # covert_channels = self._detect_covert_channels(pcap_path) # Placeholder
            # anomalies.extend(covert_channels)
            
            # Sort by severity
            anomalies.sort(key=lambda x: x.get('severity_score', 0), reverse=True)
            
            logger.info(f"Detected {len(anomalies)} protocol anomalies")
            return anomalies
            
        except Exception as e:
            logger.error(f"Protocol anomaly detection failed: {e}", exc_info=True)
            return []

    def extract_network_iocs(self, analysis_results: Dict[str, Any]) -> Dict[str, List[str]]:
        """
        Extract network-based Indicators of Compromise from analysis results.
        
        Compiles network IOCs:
        - IP addresses (C2, exfiltration destinations)
        - Domain names (malicious, DGA, C2)
        - URLs (malicious, C2 endpoints)
        - User agents (malicious, custom)
        - SSL/TLS certificates (malicious, self-signed - by fingerprint)
        - JA3 Hashes
        
        Args:
            analysis_results (Dict[str, Any]): Complete network analysis results
            
        Returns:
            Dict[str, List[str]]: Categorized network IOCs for threat intelligence
        """
        iocs = {
            'ip_addresses': set(),
            'domains': set(),
            'urls': set(),
            'user_agents': set(),
            'certificate_sha256': set(), # Storing SHA256 hash of suspicious certs
            'ja3_hashes': set(),
            'file_hashes': set(), # Placeholder, network module doesn't typically extract file hashes directly
            'email_addresses': set() # Placeholder, network module doesn't typically extract emails directly
        }
        
        try:
            # Extract from C2 communications
            for c2_comm in analysis_results.get('c2_communications', []):
                if c2_comm.get('destination_ip'):
                    iocs['ip_addresses'].add(c2_comm['destination_ip'])
                if c2_comm.get('domain'): # If C2 used a domain
                    iocs['domains'].add(c2_comm['domain'])
                if c2_comm.get('destination'): # General destination field
                    try: # Check if it's an IP
                        ipaddress.ip_address(c2_comm['destination'])
                        iocs['ip_addresses'].add(c2_comm['destination'])
                    except ValueError: # Assume it's a domain
                        if '.' in c2_comm['destination']: # Basic check for domain like structure
                             iocs['domains'].add(c2_comm['destination'])
                # Add suspicious URIs from C2 indicators if available
                for uri_info in c2_comm.get('suspicious_uris', []):
                    if isinstance(uri_info, str): # if URI is a string
                        # Construct full URL if possible
                        host_for_url = c2_comm.get('destination_ip') or c2_comm.get('domain') or c2_comm.get('destination')
                        if host_for_url:
                             iocs['urls'].add(f"http://{host_for_url}{uri_info}") # Assume http for simplicity
                    elif isinstance(uri_info, dict) and uri_info.get('uri'): # if URI is a dict with 'uri' key
                        host_for_url = c2_comm.get('destination_ip') or c2_comm.get('domain') or c2_comm.get('destination')
                        if host_for_url:
                            iocs['urls'].add(f"http://{host_for_url}{uri_info['uri']}")


            # Extract domains from DGA detection
            for dga_domain_info in analysis_results.get('dga_domains', []):
                iocs['domains'].add(dga_domain_info['domain'])
            
            # Extract from HTTP analysis
            http_analysis = analysis_results.get('http_analysis', {})
            for sus_url_info in http_analysis.get('suspicious_urls', []):
                iocs['urls'].add(sus_url_info['url'])
                if sus_url_info.get('host'):
                    iocs['domains'].add(sus_url_info['host'])
            
            for request in http_analysis.get('suspicious_requests', []):
                if request.get('user_agent'):
                    iocs['user_agents'].add(request['user_agent'])
                if request.get('host'):
                     iocs['domains'].add(request.get('host'))
                # Add IP from suspicious requests
                if request.get('dst_ip'):
                    iocs['ip_addresses'].add(request.get('dst_ip'))


            # Extract from encrypted traffic analysis
            encrypted_traffic = analysis_results.get('encrypted_traffic', {})
            for cert_event in encrypted_traffic.get('suspicious_certificates', []):
                cert_detail = cert_event.get('certificate', {})
                if cert_detail.get('sha256_hash'):
                    iocs['certificate_sha256'].add(cert_detail['sha256_hash'])
                if cert_event.get('destination_ip'):
                    iocs['ip_addresses'].add(cert_event['destination_ip'])
                # Add SNI as a domain IOC if present
                sni = cert_event.get('server_name_indication')
                if sni and '.' in sni: # Basic check for domain
                    iocs['domains'].add(sni)

            for ja3_hash in encrypted_traffic.get('ja3_fingerprints', {}).keys():
                iocs['ja3_hashes'].add(ja3_hash)
            
            # Extract from data exfiltration
            for exfil_event in analysis_results.get('data_exfiltration', []):
                if exfil_event.get('destination'): # Could be IP or domain
                    try:
                        ipaddress.ip_address(exfil_event['destination'])
                        iocs['ip_addresses'].add(exfil_event['destination'])
                    except ValueError:
                        if '.' in exfil_event['destination']:
                             iocs['domains'].add(exfil_event['destination'])
                if exfil_event.get('domain'): # Specifically if domain is identified for DNS tunneling
                    iocs['domains'].add(exfil_event['domain'])
            
            # Extract from suspicious DNS queries
            dns_analysis = analysis_results.get('dns_analysis', {})
            for query in dns_analysis.get('suspicious_queries', []):
                if query.get('domain'):
                    iocs['domains'].add(query['domain'])
                # Resolved IPs from suspicious DNS queries could also be IOCs if available
                # This would require parsing DNS responses, not just queries.
            
            # Convert sets to lists and filter out empty values
            final_iocs = {}
            for category, ioc_set in iocs.items():
                final_iocs[category] = sorted(list(filter(None, ioc_set))) # Sort for consistent output
            
            logger.info(f"Extracted {sum(len(v) for v in final_iocs.values())} network IOCs")
            return final_iocs
            
        except Exception as e:
            logger.error(f"IOC extraction failed: {e}", exc_info=True)
            # Return empty structure on failure
            return {category: [] for category in iocs.keys()}

    def generate_traffic_summary(self, pcap_path: Path) -> Dict[str, Any]:
        """
        Generate a comprehensive summary of network traffic.
        
        Creates overview including:
        - Total packets and bytes
        - Protocol distribution
        - Top talkers (IPs, ports)
        - Geographic distribution (placeholder)
        - Timeline of activity (start, end, duration)
        
        Args:
            pcap_path (Path): Path to the PCAP file
            
        Returns:
            Dict[str, Any]: Traffic summary statistics and metadata
        """
        summary = {
            'total_packets': 0,
            'total_bytes': 0,
            'duration_seconds': 0.0,
            'start_time_epoch': None,
            'end_time_epoch': None,
            'protocols': Counter(),
            'top_source_ips': Counter(),
            'top_dest_ips': Counter(),
            'top_source_ports': Counter(),
            'top_dest_ports': Counter(),
            'packet_sizes_stats': {}, # avg, min, max, stddev
            'total_connections': 0, # Estimate if possible
            'unique_ips': set(),
            'internal_ips': set(),
            'external_ips': set()
        }
        
        try:
            if SCAPY_AVAILABLE:
                # Scapy can be memory intensive for large files, consider iterative processing if needed
                summary = self._generate_summary_with_scapy(pcap_path)
            elif DPKT_AVAILABLE:
                summary = self._generate_summary_with_dpkt(pcap_path)
            else:
                # Fallback to tshark (can be slow for full summary)
                summary = self._generate_summary_with_tshark(pcap_path) # This is a basic tshark summary
            
            # Calculate derived statistics if not already done by helpers
            if summary.get('total_packets', 0) > 0 and summary.get('packet_sizes'): # packet_sizes might be a list
                packet_sizes_list = summary['packet_sizes']
                if packet_sizes_list: # Ensure it's not empty
                    summary['packet_sizes_stats']['avg'] = statistics.mean(packet_sizes_list)
                    summary['packet_sizes_stats']['min'] = min(packet_sizes_list)
                    summary['packet_sizes_stats']['max'] = max(packet_sizes_list)
                    if len(packet_sizes_list) > 1:
                        summary['packet_sizes_stats']['stddev'] = statistics.stdev(packet_sizes_list)
                    else:
                        summary['packet_sizes_stats']['stddev'] = 0
                summary.pop('packet_sizes', None) # Remove raw list after stats

            summary['unique_ip_count'] = len(summary.get('unique_ips', set()))
            summary['internal_ip_count'] = len(summary.get('internal_ips', set()))
            summary['external_ip_count'] = len(summary.get('external_ips', set()))
            
            # Convert sets to sorted lists for JSON serialization and consistent output
            for key in ['unique_ips', 'internal_ips', 'external_ips']:
                if key in summary and isinstance(summary[key], set):
                    summary[key] = sorted(list(summary[key]))
            
            # Convert Counters to dicts of top N for cleaner output
            for key in ['top_source_ips', 'top_dest_ips', 'top_source_ports', 'top_dest_ports', 'protocols']:
                if key in summary and isinstance(summary[key], Counter):
                    summary[key] = dict(summary[key].most_common(10))


            logger.info(f"Generated traffic summary: {summary.get('total_packets',0)} packets, {summary.get('total_bytes',0)} bytes")
            return summary
            
        except Exception as e:
            logger.error(f"Traffic summary generation failed: {e}", exc_info=True)
            # Return the initialized summary (mostly empty) if an error occurs
            return summary

    def correlate_with_behavioral_data(self, network_results: Dict, behavioral_results: Dict) -> Dict[str, Any]:
        """
        Correlate network activity with behavioral analysis results. (Placeholder)
        
        Links network communications with:
        - Process activity (which process made connections)
        - File access patterns (data accessed before transmission)
        - Timeline correlation (sequence of events)
        - Attribution (linking network activity to malware components)
        
        Args:
            network_results (Dict): Network analysis results
            behavioral_results (Dict): Behavioral analysis results
            
        Returns:
            Dict[str, Any]: Correlated analysis showing relationships between behaviors
        """
        correlation = {
            'process_network_mapping': [],
            'file_to_network_correlation': [],
            'timeline_correlation': [],
            'attribution_analysis': {},
            'confidence_scores': {}
        }
        
        try:
            # This is a placeholder for a complex correlation logic.
            # Example: Correlate network connections with process activity
            # behavioral_processes = behavioral_results.get('detected_techniques', {}).get('processes', [])
            # network_connections = network_results.get('traffic_summary', {}).get('connections', []) # Assuming connections are listed
            
            # for process in behavioral_processes:
            #     process_name = process.get('name', '')
            #     related_connections = []
                
            #     # Find network connections that might be related to this process
            #     # This would require timestamps, PIDs (if available from netstat-like data in behavioral)
            #     # and matching them to connection timestamps and potentially local ports.
            #     for conn_type, connections_list in network_results.items():
            #         if isinstance(connections_list, list): # e.g. http_analysis['suspicious_requests']
            #             for conn_detail in connections_list:
            #                 if self._is_connection_related_to_process(conn_detail, process, behavioral_results):
            #                     related_connections.append({
            #                         'connection_type': conn_type,
            #                         'connection_details': conn_detail,
            #                         'confidence': self._calculate_correlation_confidence(conn_detail, process)
            #                     })
                
            #     if related_connections:
            #         correlation['process_network_mapping'].append({
            #             'process_details': process,
            #             'associated_network_activity': related_connections
            #         })
            
            logger.info("Placeholder: Completed network-behavioral correlation analysis")
            return correlation
            
        except Exception as e:
            logger.error(f"Correlation analysis failed: {e}", exc_info=True)
            return correlation
    
    # --- Helper methods for PCAP parsing with different libraries ---
    def _extract_dns_with_scapy(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract DNS queries using Scapy"""
        dns_queries = []
        if not SCAPY_AVAILABLE:
            logger.warning("Scapy is not available, cannot extract DNS queries with Scapy.")
            return dns_queries
        try:
            packets = scapy.rdpcap(str(pcap_path))
            for packet in packets:
                if packet.haslayer(DNS) and packet[DNS].qr == 0 and packet.haslayer(DNSQR):  # DNS query
                    try:
                        domain_name = packet[DNSQR].qname.decode('utf-8', 'ignore').rstrip('.')
                        query = {
                            'timestamp': float(packet.time),
                            'src_ip': packet[scapy.IP].src if packet.haslayer(scapy.IP) else '',
                            'dst_ip': packet[scapy.IP].dst if packet.haslayer(scapy.IP) else '',
                            'domain': domain_name,
                            'query_type': packet[DNSQR].qtype,
                            'query_class': packet[DNSQR].qclass
                        }
                        domain_parts = domain_name.split('.')
                        # Store the first part as subdomain if it's not the whole domain (e.g. for 'example.com', subdomain is 'example')
                        # For 'com', subdomain is empty. For 'www.example.com', subdomain is 'www'.
                        query['subdomain'] = domain_parts[0] if len(domain_parts) > 1 else ''
                        dns_queries.append(query)
                    except Exception as e_inner:
                        logger.debug(f"Error processing individual Scapy DNS packet: {e_inner}")
                        continue
        except Exception as e:
            logger.warning(f"Scapy DNS extraction error: {e}")
        return dns_queries
    
    def _extract_dns_with_dpkt(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract DNS queries using dpkt"""
        dns_queries = []
        if not DPKT_AVAILABLE:
            logger.warning("dpkt is not available, cannot extract DNS queries with dpkt.")
            return dns_queries
        try:
            with open(pcap_path, 'rb') as f:
                pcap_reader = dpkt.pcap.Reader(f)
                for timestamp, buf in pcap_reader:
                    try:
                        eth = dpkt.ethernet.Ethernet(buf)
                        if not isinstance(eth.data, dpkt.ip.IP):
                            continue
                        
                        ip = eth.data
                        # Check for UDP (DNS typically uses UDP, but can use TCP)
                        if isinstance(ip.data, dpkt.udp.UDP):
                            udp = ip.data
                            if udp.dport == 53 or udp.sport == 53: # DNS port
                                try:
                                    dns = dpkt.dns.DNS(udp.data)
                                    if dns.qr == dpkt.dns.DNS_Q and dns.qd:  # Query with questions
                                        for question in dns.qd:
                                            domain_name = question.name
                                            query = {
                                                'timestamp': timestamp,
                                                'src_ip': self._ip_to_str(ip.src),
                                                'dst_ip': self._ip_to_str(ip.dst),
                                                'domain': domain_name,
                                                'query_type': question.type,
                                                'query_class': question.cls
                                            }
                                            domain_parts = domain_name.split('.')
                                            query['subdomain'] = domain_parts[0] if len(domain_parts) > 1 else ''
                                            dns_queries.append(query)
                                except (dpkt.dpkt.NeedData, dpkt.dpkt.UnpackError, AttributeError):
                                    logger.debug("dpkt: Could not unpack DNS data from UDP packet.")
                                    continue
                        # Optionally, add TCP DNS support here if needed
                    except Exception as e_inner: # Catch errors processing individual packets
                        logger.debug(f"Error processing individual dpkt packet for DNS: {e_inner}")
                        continue
        except Exception as e:
            logger.warning(f"dpkt DNS extraction error: {e}")
        return dns_queries

    def _extract_dns_with_pyshark(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract DNS queries using PyShark"""
        dns_queries = []
        if not PYSHARK_AVAILABLE:
            logger.warning("PyShark is not available, cannot extract DNS queries with PyShark.")
            return dns_queries
        try:
            # Filter for DNS queries (flags.response == 0)
            cap = pyshark.FileCapture(str(pcap_path), display_filter='dns.flags.response == 0')
            for packet in cap:
                try:
                    if hasattr(packet, 'dns') and hasattr(packet.dns, 'qry_name'):
                        domain_name = packet.dns.qry_name
                        query = {
                            'timestamp': float(packet.sniff_timestamp),
                            'src_ip': packet.ip.src if hasattr(packet, 'ip') else '',
                            'dst_ip': packet.ip.dst if hasattr(packet, 'ip') else '',
                            'domain': domain_name,
                            'query_type': int(packet.dns.qry_type) if hasattr(packet.dns, 'qry_type') else 1,
                            'query_class': int(packet.dns.qry_class) if hasattr(packet.dns, 'qry_class') else 1
                        }
                        domain_parts = domain_name.split('.')
                        query['subdomain'] = domain_parts[0] if len(domain_parts) > 1 else ''
                        dns_queries.append(query)
                except Exception as e_inner:
                    logger.debug(f"Error processing individual PyShark DNS packet: {e_inner}")
                    continue
            cap.close()
        except Exception as e: # Catch pyshark.capture.capture.TSharkCrashException or other FileCapture errors
            logger.warning(f"PyShark DNS extraction error: {e}")
        return dns_queries

    def _extract_dns_with_tshark(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract DNS queries using tshark command-line tool"""
        dns_queries = []
        try:
            # Ensure tshark is available
            subprocess.run(['tshark', '-v'], capture_output=True, check=False) # Simple check
        except FileNotFoundError:
            logger.warning("tshark command not found. Cannot extract DNS with tshark.")
            return dns_queries

        try:
            cmd = [
                'tshark', '-r', str(pcap_path),
                '-Y', 'dns.flags.response == 0 && dns.qry.name', # Ensure query name exists
                '-T', 'fields',
                '-e', 'frame.time_epoch',
                '-e', 'ip.src',
                '-e', 'ip.dst',
                '-e', 'dns.qry.name',
                '-e', 'dns.qry.type',
                '-E', 'separator=|', '-E', 'occurrence=f' # First occurrence for multi-field lines
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line:
                        parts = line.split('|')
                        if len(parts) >= 4: # Need at least timestamp, src, dst, domain
                            try:
                                ts_str, src_ip_str, dst_ip_str, domain_val = parts[0], parts[1], parts[2], parts[3]
                                q_type_str = parts[4] if len(parts) > 4 and parts[4] else '1'
                                
                                query_type_val = 1 # Default A record type
                                if q_type_str.strip():
                                    try:
                                        query_type_val = int(q_type_str)
                                    except ValueError:
                                        logger.debug(f"tshark: Could not parse query type '{q_type_str}', defaulting to 1.")
                                
                                query = {
                                    'timestamp': float(ts_str) if ts_str else 0.0,
                                    'src_ip': src_ip_str,
                                    'dst_ip': dst_ip_str,
                                    'domain': domain_val,
                                    'query_type': query_type_val
                                }
                                domain_parts = domain_val.split('.')
                                query['subdomain'] = domain_parts[0] if len(domain_parts) > 1 else ''
                                dns_queries.append(query)
                            except (ValueError, IndexError) as e_parse:
                                logger.debug(f"tshark: Error parsing line '{line}': {e_parse}")
                                continue
            else:
                logger.warning(f"tshark command failed with error: {result.stderr}")
        except subprocess.TimeoutExpired:
            logger.warning("tshark DNS extraction timed out.")
        except Exception as e:
            logger.warning(f"tshark DNS extraction error: {e}")
        return dns_queries
    
    def _analyze_dns_query(self, query: Dict[str, Any]):
        """Analyze individual DNS query for suspicious characteristics"""
        domain = query.get('domain', '')
        if not domain: # Skip if no domain
            query['suspicious'] = False
            query['suspicion_reasons'] = []
            return

        # Check against known malicious domains
        if domain in self.known_malicious_domains:
            query['suspicious'] = True
            query['suspicion_reasons'] = ['known_malicious_domain']
            return # Early exit if known malicious
        
        suspicion_reasons = []
        
        # Check for suspicious TLD
        if any(domain.endswith(tld) for tld in self.suspicious_tlds):
            suspicion_reasons.append('suspicious_tld')
        
        # Analyze subdomain characteristics (the part before the main domain, e.g., 'www' in 'www.example.com')
        # This 'subdomain' key is now added by extraction methods.
        subdomain_part = query.get('subdomain', '') # Use the extracted subdomain
        if not subdomain_part: # If no distinct subdomain part, analyze the whole domain name's first part
            domain_parts_for_analysis = domain.split('.')
            subdomain_part_for_analysis = domain_parts_for_analysis[0] if domain_parts_for_analysis else ''
        else:
            subdomain_part_for_analysis = subdomain_part

        if subdomain_part_for_analysis:
            if len(subdomain_part_for_analysis) > 30:  # Very long first part of domain/subdomain
                suspicion_reasons.append('long_subdomain_part')
            
            entropy = self._calculate_entropy(subdomain_part_for_analysis)
            query['entropy'] = entropy # Store entropy for reference
            if entropy > self.dga_params.get('min_entropy', 3.5) + 0.5 :  # Slightly higher threshold for general suspicion
                suspicion_reasons.append('high_entropy_subdomain_part')
        
        # Check for numeric domains (e.g. IP address used as domain name)
        # This regex checks if the domain *is* an IP, not if it contains one.
        if re.fullmatch(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', domain):
            suspicion_reasons.append('ip_as_domain')
        
        # Check for DGA characteristics using the full domain
        dga_score = self._calculate_dga_score(domain)
        query['dga_score'] = dga_score
        if dga_score > 0.6: # Higher threshold for adding to general suspicion reasons
            suspicion_reasons.append('dga_characteristics')
        
        query['suspicious'] = len(suspicion_reasons) > 0
        query['suspicion_reasons'] = suspicion_reasons

    def _calculate_dga_score(self, domain: str) -> float:
        """Calculate DGA likelihood score for a domain"""
        if not domain or '.' not in domain:
            return 0.0
        
        score = 0.0
        # DGA characteristics are often in the first label (e.g., [randomstring].dga-provider.com)
        first_label = domain.split('.')[0]
        
        if not first_label: # Should not happen if domain is valid
            return 0.0

        # Calculate entropy for the first label
        entropy = self._calculate_entropy(first_label)
        if entropy > self.dga_params['min_entropy']:
            score += 0.3
        
        # Check vowel ratio for the first label
        if len(first_label) > 0: # Avoid division by zero
            vowels = sum(1 for c in first_label.lower() if c in 'aeiou')
            vowel_ratio = vowels / len(first_label)
            # Low vowel ratio can be an indicator (many DGA domains are consonant-heavy)
            if vowel_ratio < self.dga_params['max_vowel_ratio'] and len(first_label) > 5: # Apply if label is somewhat long
                 score += 0.2
            # Or very high vowel ratio (less common for DGA but can be odd)
            elif vowel_ratio > 0.7 and len(first_label) > 5:
                 score += 0.1


        # Check for consonant clusters in the first label
        consonant_clusters = len(re.findall(r'[bcdfghjklmnpqrstvwxyz]{3,}', first_label.lower()))
        if consonant_clusters >= self.dga_params['min_consonant_clusters']:
            score += 0.2
        
        # Check length of the first label
        if len(first_label) >= self.dga_params['min_domain_length']:
            score += 0.1
        elif len(first_label) > 20: # Extra score for very long first labels
            score += 0.15

        # Check for dictionary words (simplified) - low match suggests randomness
        # This is a very basic check and might need a proper dictionary for accuracy.
        common_words = ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'man', 'run', 'she', 'try', 'use', 'mail', 'web', 'server', 'online', 'site']
        
        # Check how much of the first_label is made of dictionary words (very simplified)
        matched_chars = 0
        for word in common_words:
            if word in first_label.lower():
                matched_chars += len(word)
        
        dict_ratio = matched_chars / len(first_label) if len(first_label) > 0 else 0
        if dict_ratio < self.dga_params['max_dictionary_ratio']: # Low ratio of dictionary words
            score += 0.2
        
        return min(score, 1.0) # Cap score at 1.0

    def _calculate_entropy(self, text: str) -> float:
        """Calculate Shannon entropy of a string"""
        if not text:
            return 0.0
        
        # Count character frequencies
        char_counts = Counter(text) # Case-sensitive entropy can be more indicative for DGAs
        text_len = len(text)
        
        # Calculate entropy
        entropy = 0.0
        for count in char_counts.values():
            probability = count / text_len
            entropy -= probability * math.log2(probability)
        
        return entropy

    def _analyze_domain_characteristics(self, domain: str) -> Dict[str, Any]:
        """Analyze various characteristics of a domain"""
        if not domain:
            return {}
        
        parts = domain.split('.')
        first_label = parts[0] if parts else '' # First label, e.g., 'www' or 'randomdga'
        
        characteristics = {
            'length': len(domain),
            'label_count': len(parts),
            'first_label_length': len(first_label),
            'entropy_first_label': self._calculate_entropy(first_label),
            'tld': parts[-1] if len(parts) > 1 else (parts[0] if len(parts) == 1 else '')
        }

        if first_label: # Avoid division by zero if first_label is empty
            characteristics['digit_ratio_first_label'] = sum(1 for c in first_label if c.isdigit()) / len(first_label)
            characteristics['vowel_ratio_first_label'] = sum(1 for c in first_label.lower() if c in 'aeiou') / len(first_label)
            characteristics['consonant_clusters_first_label'] = len(re.findall(r'[bcdfghjklmnpqrstvwxyz]{3,}', first_label.lower()))
            characteristics['repeating_chars_first_label'] = len(re.findall(r'(.)\1{2,}', first_label.lower())) # e.g. aaa
        else:
            characteristics['digit_ratio_first_label'] = 0.0
            characteristics['vowel_ratio_first_label'] = 0.0
            characteristics['consonant_clusters_first_label'] = 0
            characteristics['repeating_chars_first_label'] = 0
            
        return characteristics

    def _extract_http_connections(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract HTTP request details from PCAP. Name implies connections, but it extracts requests."""
        requests = []
        # Prioritize Scapy or dpkt if available, then tshark as fallback
        try:
            if SCAPY_AVAILABLE:
                requests = self._extract_http_with_scapy(pcap_path)
            elif DPKT_AVAILABLE:
                requests = self._extract_http_with_dpkt(pcap_path)
            elif PYSHARK_AVAILABLE: # Adding PyShark as an option
                requests = self._extract_http_with_pyshark(pcap_path)
            else:
                requests = self._extract_http_with_tshark(pcap_path)
        except Exception as e:
            logger.warning(f"HTTP extraction error using chosen method: {e}")
            # Optionally try another method if the primary one fails, or just return empty.
        return requests

    def _extract_http_with_scapy(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract HTTP requests using Scapy"""
        requests_data = []
        if not SCAPY_AVAILABLE:
            logger.warning("Scapy not available for HTTP extraction.")
            return requests_data
        try:
            packets = scapy.rdpcap(str(pcap_path))
            for packet in packets:
                if packet.haslayer(scapy.TCP) and packet.haslayer(scapy.Raw):
                    try:
                        payload = packet[scapy.Raw].load
                        # Basic check for HTTP request methods
                        # More robust parsing would involve a proper HTTP parser
                        payload_str = payload.decode('utf-8', errors='ignore')
                        if payload_str.startswith(('GET ', 'POST ', 'PUT ', 'DELETE ', 'HEAD ', 'OPTIONS ', 'PATCH ')):
                            parsed_request = self._parse_http_request(payload_str) # Pass string
                            if parsed_request:
                                parsed_request.update({
                                    'timestamp': float(packet.time),
                                    'src_ip': packet[scapy.IP].src if packet.haslayer(scapy.IP) else '',
                                    'dst_ip': packet[scapy.IP].dst if packet.haslayer(scapy.IP) else '',
                                    'src_port': packet[scapy.TCP].sport,
                                    'dst_port': packet[scapy.TCP].dport,
                                    'payload_length': len(payload) # Raw payload length
                                })
                                requests_data.append(parsed_request)
                    except Exception as e_inner:
                        logger.debug(f"Scapy: Error parsing potential HTTP packet: {e_inner}")
                        continue
        except Exception as e:
            logger.warning(f"Scapy HTTP extraction error: {e}")
        return requests_data

    def _extract_http_with_dpkt(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract HTTP requests using dpkt"""
        requests_data = []
        if not DPKT_AVAILABLE:
            logger.warning("dpkt not available for HTTP extraction.")
            return requests_data
        try:
            with open(pcap_path, 'rb') as f:
                pcap_reader = dpkt.pcap.Reader(f)
                for timestamp, buf in pcap_reader:
                    try:
                        eth = dpkt.ethernet.Ethernet(buf)
                        if not isinstance(eth.data, dpkt.ip.IP):
                            continue
                        
                        ip = eth.data
                        if not isinstance(ip.data, dpkt.tcp.TCP):
                            continue
                        
                        tcp = ip.data
                        # Check for common HTTP ports, or if data exists
                        if (tcp.dport == 80 or tcp.sport == 80 or tcp.dport == 8080 or tcp.sport == 8080) and len(tcp.data) > 0:
                            try:
                                http_req = dpkt.http.Request(tcp.data)
                                request_info = {
                                    'timestamp': timestamp,
                                    'src_ip': self._ip_to_str(ip.src),
                                    'dst_ip': self._ip_to_str(ip.dst),
                                    'src_port': tcp.sport,
                                    'dst_port': tcp.dport,
                                    'method': http_req.method,
                                    'uri': http_req.uri,
                                    'host': http_req.headers.get('host', ''),
                                    'user_agent': http_req.headers.get('user-agent', ''),
                                    'content_type': http_req.headers.get('content-type', ''),
                                    'content_length': int(http_req.headers.get('content-length', 0)),
                                    'payload_length': len(http_req.body)
                                }
                                requests_data.append(request_info)
                            except (dpkt.dpkt.NeedData, dpkt.dpkt.UnpackError, AttributeError):
                                # Not a valid HTTP request or unable to parse
                                logger.debug("dpkt: Could not parse HTTP request from TCP segment.")
                                continue
                    except Exception as e_inner:
                        logger.debug(f"dpkt: Error processing packet for HTTP: {e_inner}")
                        continue
        except Exception as e:
            logger.warning(f"dpkt HTTP extraction error: {e}")
        return requests_data
    
    def _extract_http_with_pyshark(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract HTTP requests using PyShark."""
        requests_data = []
        if not PYSHARK_AVAILABLE:
            logger.warning("PyShark not available for HTTP extraction.")
            return requests_data
        try:
            cap = pyshark.FileCapture(str(pcap_path), display_filter='http.request')
            for packet in cap:
                try:
                    if hasattr(packet, 'http'):
                        http_layer = packet.http
                        host = getattr(http_layer, 'host', '')
                        if not host and hasattr(http_layer, 'request_uri_host'): # Try another field if host is empty
                             host = getattr(http_layer, 'request_uri_host', '')

                        request_info = {
                            'timestamp': float(packet.sniff_timestamp),
                            'src_ip': packet.ip.src if hasattr(packet, 'ip') else '',
                            'dst_ip': packet.ip.dst if hasattr(packet, 'ip') else '',
                            'src_port': int(packet.tcp.srcport) if hasattr(packet, 'tcp') else 0,
                            'dst_port': int(packet.tcp.dstport) if hasattr(packet, 'tcp') else 0,
                            'method': getattr(http_layer, 'request_method', ''),
                            'uri': getattr(http_layer, 'request_uri', ''),
                            'host': host,
                            'user_agent': getattr(http_layer, 'user_agent', ''),
                            'content_type': getattr(http_layer, 'content_type', ''),
                            'content_length': int(getattr(http_layer, 'content_length_header', 0)), # HTTP header
                            'payload_length': int(getattr(http_layer, 'file_data_len', 0)) # If file data is identified
                        }
                        requests_data.append(request_info)
                except Exception as e_inner:
                    logger.debug(f"PyShark: Error processing HTTP packet: {e_inner}")
                    continue
            cap.close()
        except Exception as e:
            logger.warning(f"PyShark HTTP extraction error: {e}")
        return requests_data

    def _extract_http_with_tshark(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract HTTP requests using tshark"""
        requests_data = []
        try:
            subprocess.run(['tshark', '-v'], capture_output=True, check=False)
        except FileNotFoundError:
            logger.warning("tshark command not found. Cannot extract HTTP with tshark.")
            return requests_data
        try:
            cmd = [
                'tshark', '-r', str(pcap_path),
                '-Y', 'http.request', # Filter for HTTP requests
                '-T', 'fields',
                '-e', 'frame.time_epoch',
                '-e', 'ip.src', '-e', 'ip.dst',
                '-e', 'tcp.srcport', '-e', 'tcp.dstport',
                '-e', 'http.request.method',
                '-e', 'http.request.full_uri', # Often more complete than just .uri
                '-e', 'http.host',
                '-e', 'http.user_agent',
                '-e', 'http.content_type',
                '-e', 'http.content_length', # Content-Length header
                '-E', 'separator=|', '-E', 'occurrence=f'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line:
                        parts = line.split('|')
                        if len(parts) >= 7: # Minimum fields for a basic request
                            try:
                                request_info = {
                                    'timestamp': float(parts[0]) if parts[0] else 0.0,
                                    'src_ip': parts[1], 'dst_ip': parts[2],
                                    'src_port': int(parts[3]) if parts[3] else 0,
                                    'dst_port': int(parts[4]) if parts[4] else 0,
                                    'method': parts[5],
                                    'url': parts[6], # tshark calls it full_uri
                                    'host': parts[7] if len(parts) > 7 and parts[7] else '',
                                    'user_agent': parts[8] if len(parts) > 8 and parts[8] else '',
                                    'content_type': parts[9] if len(parts) > 9 and parts[9] else '',
                                    'content_length': int(parts[10]) if len(parts) > 10 and parts[10] else 0,
                                }
                                # Extract URI from full_uri if host is present
                                if request_info['host'] and request_info['url'].startswith(f"http://{request_info['host']}"):
                                    request_info['uri'] = request_info['url'][len(f"http://{request_info['host']}"):]
                                elif request_info['host'] and request_info['url'].startswith(f"https://{request_info['host']}"):
                                     request_info['uri'] = request_info['url'][len(f"https://{request_info['host']}"):]
                                else:
                                    request_info['uri'] = request_info['url'] # Fallback

                                requests_data.append(request_info)
                            except (ValueError, IndexError) as e_parse:
                                logger.debug(f"tshark: Error parsing HTTP line '{line}': {e_parse}")
                                continue
            else:
                logger.warning(f"tshark HTTP command failed: {result.stderr}")
        except subprocess.TimeoutExpired:
            logger.warning("tshark HTTP extraction timed out.")
        except Exception as e:
            logger.warning(f"tshark HTTP extraction error: {e}")
        return requests_data

    def _parse_http_request(self, payload_str: str) -> Optional[Dict[str, Any]]:
        """Parse HTTP request from raw payload string"""
        try:
            lines = payload_str.split('\r\n')
            if not lines:
                return None
            
            # Parse request line
            request_line = lines[0]
            parts = request_line.split(' ')
            if len(parts) < 2: # Need at least method and URI
                return None
            
            method = parts[0]
            uri = parts[1]
            # version = parts[2] if len(parts) > 2 else 'HTTP/1.1' # Default or parse
            
            # Parse headers
            headers = {}
            for line in lines[1:]:
                if not line: # End of headers
                    break
                if ':' in line:
                    key, value = line.split(':', 1)
                    headers[key.strip().lower()] = value.strip()
            
            return {
                'method': method,
                'uri': uri,
                # 'http_version': version,
                'host': headers.get('host', ''),
                'user_agent': headers.get('user-agent', ''),
                'content_type': headers.get('content-type', ''),
                'content_length': int(headers.get('content-length', 0))
            }
        except Exception as e:
            logger.debug(f"Error parsing raw HTTP request payload: {e}")
            return None

    def _analyze_c2_patterns(self, destination: str, connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze connections to a destination for C2 patterns"""
        c2_indicators = {
            'destination': destination, # Can be IP or domain
            'connection_count': len(connections),
            'confidence_score': 0.0,
            'indicators': [],
            'beaconing_detected': False,
            'suspicious_user_agents': set(), # Use set to store unique UAs
            'suspicious_uris': set(), # Use set to store unique URIs
            'timing_analysis': {}
        }
        
        if not connections: # No connections to analyze
            return c2_indicators

        # Check for beaconing behavior from HTTP requests
        timestamps = sorted([conn.get('timestamp', 0.0) for conn in connections if conn.get('timestamp')])
        if len(timestamps) >= 3: # Need at least 3 points for interval analysis
            intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]
            if intervals: # Ensure intervals list is not empty
                avg_interval = statistics.mean(intervals)
                std_dev = statistics.stdev(intervals) if len(intervals) > 1 else 0.0
                
                # Check if intervals are regular (low standard deviation relative to average)
                # and if average interval falls within common beaconing ranges
                if std_dev < avg_interval * 0.25 and any(abs(avg_interval - b_int) < b_int * 0.1 for b_int in self.beacon_intervals):
                    c2_indicators['beaconing_detected'] = True
                    c2_indicators['confidence_score'] += 0.4
                    c2_indicators['indicators'].append('regular_beaconing_interval')
                    c2_indicators['timing_analysis'] = {
                        'avg_interval_seconds': avg_interval,
                        'std_dev_seconds': std_dev,
                        'intervals_observed': intervals[:5] # Sample of intervals
                    }
        
        # Check for suspicious user agents and URIs from the connections
        for conn in connections:
            user_agent = conn.get('user_agent', '')
            if user_agent and any(mal_ua.lower() in user_agent.lower() for mal_ua in self.malicious_user_agents):
                c2_indicators['suspicious_user_agents'].add(user_agent)

            uri = conn.get('uri', '') # URI from HTTP request
            if uri and any(re.search(pattern, uri, re.IGNORECASE) for pattern in self.suspicious_uri_patterns):
                c2_indicators['suspicious_uris'].add(uri)

        if c2_indicators['suspicious_user_agents']:
            c2_indicators['confidence_score'] += 0.2
            c2_indicators['indicators'].append('suspicious_user_agent_found')
        if c2_indicators['suspicious_uris']:
            c2_indicators['confidence_score'] += 0.3
            c2_indicators['indicators'].append('suspicious_uri_pattern_found')
        
        # Convert sets to lists for JSON output
        c2_indicators['suspicious_user_agents'] = list(c2_indicators['suspicious_user_agents'])
        c2_indicators['suspicious_uris'] = list(c2_indicators['suspicious_uris'])


        # Check for known malicious destination
        if destination in self.known_malicious_ips or destination in self.known_malicious_domains:
            c2_indicators['confidence_score'] += 0.5
            c2_indicators['indicators'].append('known_malicious_destination')
        
        # Check connection frequency (simple heuristic)
        if len(connections) > 10 and not c2_indicators['beaconing_detected']: # High count but not beaconing
            c2_indicators['confidence_score'] += 0.1
            c2_indicators['indicators'].append('high_connection_frequency_to_destination')
        
        c2_indicators['confidence_score'] = min(c2_indicators['confidence_score'], 1.0) # Cap score
        return c2_indicators

    def _detect_beaconing_behavior(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect beaconing behavior in network traffic (generic placeholder)"""
        beaconing_comms = []
        logger.info("Placeholder: Generic beaconing detection not yet implemented.")
        # This method could analyze all flows (not just HTTP) for beaconing patterns.
        # For now, beaconing is primarily checked within _analyze_c2_patterns for HTTP.
        return beaconing_comms

    def _extract_http_requests(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Helper to call the main HTTP extraction logic."""
        return self._extract_http_connections(pcap_path)

    def _is_suspicious_http_request(self, request: Dict[str, Any]) -> bool:
        """Check if HTTP request has suspicious characteristics"""
        # Check user agent
        user_agent = request.get('user_agent', '')
        if any(mal_ua.lower() in user_agent.lower() for mal_ua in self.malicious_user_agents):
            return True
        
        # Check URI patterns
        uri = request.get('uri', '') # URI path and query
        if uri and any(re.search(pattern, uri, re.IGNORECASE) for pattern in self.suspicious_uri_patterns):
            return True
        
        # Check for unusual content lengths for GET requests (should be 0 or small)
        method = request.get('method', '')
        content_length = request.get('content_length', 0)
        if method == 'GET' and content_length > 1024: # GET with large body
            return True
        
        # Check for very large POST/PUT (already handled in analyze_http_traffic for 'large_transfers')
        # if method in ['POST', 'PUT'] and content_length > 50 * 1024 * 1024: # > 50MB
        #     return True
            
        # Check for unusual methods if not already covered by C2 patterns
        # if method in ['TRACE', 'CONNECT']: # CONNECT is used for HTTPS proxies, TRACE for debugging
        #     return True # These can be legitimate but also abused.
            
        return False

    def _is_suspicious_url(self, url: str) -> bool:
        """Check if URL (full URL) is suspicious"""
        if not url:
            return False
        
        try:
            parsed_url = urlparse(url)
            path_query = parsed_url.path
            if parsed_url.query:
                path_query += "?" + parsed_url.query
            
            # Check for suspicious patterns in path/query
            if path_query and any(re.search(pattern, path_query, re.IGNORECASE) for pattern in self.suspicious_uri_patterns):
                return True
            
            # Check for very long URLs
            if len(url) > 1024: # Adjusted length
                return True
            
            # Check for IP addresses as hostname in URL
            if re.fullmatch(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', parsed_url.hostname or ''):
                return True
            
            # Check TLD of hostname against suspicious TLDs
            if parsed_url.hostname and any(parsed_url.hostname.endswith(tld) for tld in self.suspicious_tlds):
                return True

        except Exception as e:
            logger.debug(f"Error parsing URL '{url}': {e}")
            return False # Treat as not suspicious if parsing fails
            
        return False

    def _detect_large_transfers(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect large data transfers (generic placeholder)"""
        transfers = []
        logger.info("Placeholder: Generic large transfer detection not yet implemented.")
        # This would require analyzing flows (TCP/UDP conversations) and summing up byte counts.
        # For now, large HTTP POSTs are handled in analyze_http_traffic.
        # Example structure if implemented:
        # { 'src_ip': ..., 'dest_ip': ..., 'dest_port': ..., 'protocol': ..., 
        #   'bytes_out': ..., 'bytes_in': ..., 'duration': ... }
        return transfers

    def _detect_dns_tunneling(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect DNS tunneling attempts based on query patterns."""
        tunneling_attempts = []
        try:
            dns_queries_list = self.extract_dns_queries(pcap_path) 
            
            queries_by_domain = defaultdict(list)
            for query in dns_queries_list:
                domain = query.get('domain', '')
                if domain:
                    queries_by_domain[domain].append(query)
            
            for domain, queries_for_this_domain in queries_by_domain.items():
                if len(queries_for_this_domain) < 5: # Need multiple queries for meaningful tunneling analysis
                    continue
                
                # Sort queries by timestamp to correctly calculate rate and duration
                queries_for_this_domain.sort(key=lambda q: q.get('timestamp', 0.0))

                first_ts = queries_for_this_domain[0].get('timestamp', 0.0)
                last_ts = queries_for_this_domain[-1].get('timestamp', 0.0)
                duration_seconds = (last_ts - first_ts) if last_ts > first_ts else 1.0 
                duration_seconds = max(duration_seconds, 1.0) # Ensure at least 1 second to avoid div by zero

                query_rate_per_minute = len(queries_for_this_domain) / (duration_seconds / 60.0)

                # Analyze subdomains specifically for this domain's queries
                subdomains_in_queries = [q.get('subdomain', '') for q in queries_for_this_domain if q.get('subdomain')]
                
                unique_subdomains_set = set(s for s in subdomains_in_queries if s) # Non-empty unique subdomains
                unique_subdomains_count = len(unique_subdomains_set)
                
                avg_subdomain_length = 0.0
                if subdomains_in_queries: # Calculate mean only if list is not empty
                    non_empty_subdomain_lengths = [len(s) for s in subdomains_in_queries if s]
                    if non_empty_subdomain_lengths:
                        avg_subdomain_length = statistics.mean(non_empty_subdomain_lengths)
                
                indicators = []
                confidence = 0.0
                
                # High query rate to the same parent domain
                if query_rate_per_minute > self.anomaly_thresholds.get('dns_query_rate', 100) / 5: # e.g. > 20 q/min to one domain
                    indicators.append('high_query_rate_to_domain')
                    confidence += 0.2
                
                # Many unique subdomains for the same parent domain
                if unique_subdomains_count > 10 and unique_subdomains_count > len(queries_for_this_domain) * 0.3 :
                    indicators.append('many_unique_subdomains')
                    confidence += 0.3
                
                # Long average subdomain length
                if avg_subdomain_length > 50: # Increased threshold for average
                    indicators.append('long_avg_subdomain_length')
                    confidence += 0.2
                
                # Check for base64-like patterns in the unique subdomains
                base64_subdomain_count = 0
                if unique_subdomains_set:
                     base64_subdomain_count = sum(1 for sub_str in unique_subdomains_set if sub_str and re.fullmatch(r'[A-Za-z0-9+/]{20,}=*', sub_str)) # At least 20 base64 chars

                if unique_subdomains_set and base64_subdomain_count > len(unique_subdomains_set) * 0.3: # If >30% of unique subdomains look like base64
                    indicators.append('base64_like_subdomains')
                    confidence += 0.4
                
                if confidence >= 0.5: # Overall confidence threshold for suspicion
                    tunneling_attempts.append({
                        'type': 'dns_tunneling_suspicion',
                        'domain': domain, # The parent domain being queried
                        'query_count': len(queries_for_this_domain),
                        'query_rate_per_minute': round(query_rate_per_minute, 2),
                        'unique_subdomains_count': unique_subdomains_count,
                        'avg_subdomain_length': round(avg_subdomain_length, 2),
                        # Rough estimate of data, assuming subdomains carry data and some overhead
                        'estimated_data_size_bytes': round(unique_subdomains_count * avg_subdomain_length * 0.75, 2), 
                        'confidence': min(round(confidence, 2), 1.0),
                        'indicators': indicators,
                        'first_timestamp': first_ts,
                        'last_timestamp': last_ts,
                        'duration_seconds': round(duration_seconds, 2)
                    })
                    
        except Exception as e:
            logger.error(f"DNS tunneling detection failed: {e}", exc_info=True)
        
        return tunneling_attempts

    def _detect_http_exfiltration(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect HTTP-based data exfiltration from previously analyzed HTTP requests."""
        exfiltration_activities = []
        
        # This method should ideally use results from analyze_http_traffic if already run,
        # or run it if not. For simplicity, let's assume we can re-evaluate or use stored results.
        # If self.analysis_results contains http_analysis, use it. Otherwise, re-calculate.
        if 'http_analysis' in self.analysis_results and self.analysis_results['http_analysis']:
            http_analysis_data = self.analysis_results['http_analysis']
        else:
            http_analysis_data = self.analyze_http_traffic(pcap_path) # Could be redundant if called before

        # Look for large POST requests from http_analysis_data
        for post_req in http_analysis_data.get('post_requests', []):
            content_length = post_req.get('content_length', 0)
            if content_length > 1 * 1024 * 1024:  # > 1MB POST considered large for exfil
                exfiltration_activities.append({
                    'type': 'http_large_post_exfiltration',
                    'destination_host': post_req.get('host'),
                    'url': post_req.get('url'),
                    'data_size_bytes': content_length,
                    'timestamp': post_req.get('timestamp'),
                    'confidence': 0.6, # Moderate confidence based on size
                    'details': {'content_type': post_req.get('content_type')}
                })
        
        # Look for suspicious file uploads (e.g. to known file sharing, or unusual patterns)
        upload_keywords = ['upload', 'submit', 'send', 'backup', 'archive'] # Add more keywords
        suspicious_content_types = ['application/zip', 'application/octet-stream', 'application/x-rar-compressed']

        for req in http_analysis_data.get('suspicious_requests', []) + http_analysis_data.get('post_requests', []):
            uri = req.get('uri', '').lower()
            content_type = req.get('content_type', '').lower()
            content_length = req.get('content_length', 0)

            is_suspicious_upload = False
            upload_reasons = []

            if any(keyword in uri for keyword in upload_keywords) and content_length > 100 * 1024: # >100KB
                is_suspicious_upload = True
                upload_reasons.append('uri_keyword_and_size')
            
            if content_type in suspicious_content_types and content_length > 100 * 1024:
                is_suspicious_upload = True
                upload_reasons.append('suspicious_content_type_and_size')

            if is_suspicious_upload:
                # Avoid duplicates if already added by large_post check
                existing = next((item for item in exfiltration_activities if item.get('url') == req.get('url') and item.get('timestamp') == req.get('timestamp')), None)
                if not existing:
                    exfiltration_activities.append({
                        'type': 'http_suspicious_upload',
                        'destination_host': req.get('host'),
                        'url': req.get('url'),
                        'data_size_bytes': content_length,
                        'timestamp': req.get('timestamp'),
                        'confidence': 0.4 + (0.1 * len(upload_reasons)), # Base confidence, add for more reasons
                        'details': {'content_type': content_type, 'method': req.get('method'), 'reasons': upload_reasons}
                    })
        return exfiltration_activities

    def _detect_icmp_tunneling(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect ICMP tunneling attempts (basic implementation)"""
        tunneling_attempts = []
        if not SCAPY_AVAILABLE:
            logger.warning("Scapy not available for ICMP tunneling detection.")
            return tunneling_attempts
        try:
            packets = scapy.rdpcap(str(pcap_path))
            icmp_echo_requests = []
            icmp_echo_replies = []
            
            for p in packets:
                if p.haslayer(scapy.ICMP):
                    # Focus on Echo Request/Reply which are commonly used for tunneling
                    if p[scapy.ICMP].type == 8: # Echo Request
                        icmp_echo_requests.append(p)
                    elif p[scapy.ICMP].type == 0: # Echo Reply
                        icmp_echo_replies.append(p)
            
            # Analyze Echo Requests
            if len(icmp_echo_requests) > 20: # Arbitrary threshold for "many" requests
                payload_sizes = [len(pkt[scapy.Raw].load) for pkt in icmp_echo_requests if pkt.haslayer(scapy.Raw)]
                if payload_sizes:
                    avg_payload_size = statistics.mean(payload_sizes)
                    # Standard ICMP ping payloads are small (e.g., 32 or 64 bytes). Large payloads are suspicious.
                    if avg_payload_size > 100: 
                        tunneling_attempts.append({
                            'type': 'icmp_tunneling_echo_request',
                            'packet_count': len(icmp_echo_requests),
                            'avg_payload_size_bytes': round(avg_payload_size,2),
                            'total_payload_bytes_estimate': sum(payload_sizes),
                            'confidence': 0.6,
                            'details': f"High number of ICMP Echo Requests with average payload size {avg_payload_size:.2f} bytes."
                        })
            # Similar analysis can be done for Echo Replies
            # More advanced: check for non-standard ICMP types, or data encoding in payloads.

        except Exception as e:
            logger.warning(f"ICMP tunneling detection error: {e}", exc_info=True)
        return tunneling_attempts

    def _extract_tls_connections(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Extract TLS connections and basic certificate information (simplified)."""
        tls_connections = []
        # Using tshark for TLS handshake info is generally more reliable and feature-rich
        # than trying to parse complex TLS with Scapy/dpkt directly without a dedicated TLS library.
        try:
            subprocess.run(['tshark', '-v'], capture_output=True, check=False)
        except FileNotFoundError:
            logger.warning("tshark command not found. Cannot extract full TLS details.")
            # Fallback to basic Scapy if TLS layer is available
            if SCAPY_AVAILABLE and TLS:
                logger.info("Falling back to basic Scapy for TLS info (limited).")
                return self._extract_tls_with_scapy_basic(pcap_path)
            return tls_connections

        try:
            # tshark fields for TLS analysis:
            # frame.time_epoch, ip.src, ip.dst, tcp.srcport, tcp.dstport
            # tls.handshake.type (1=ClientHello, 2=ServerHello, 11=Certificate)
            # tls.handshake.version (TLS version)
            # tls.handshake.ciphersuite (Chosen cipher suite from ServerHello)
            # tls.handshake.extension.server_name (SNI)
            # tls.handshake.certificate (Raw certificate data - harder to parse directly)
            # tls.handshake.extensions_ja3_hash (If tshark version supports it)
            # For cert details: tls.handshake.x509ce.  x509 certificate subject, issuer, validity etc.
            
            # This example focuses on JA3 and basic handshake details. Full cert parsing is complex.
            cmd = [
                'tshark', '-r', str(pcap_path),
                '-Y', 'tls.handshake.type == 1', # Client Hello for JA3
                '-T', 'fields',
                '-e', 'frame.time_epoch',
                '-e', 'ip.src', '-e', 'ip.dst',
                '-e', 'tcp.srcport', '-e', 'tcp.dstport',
                '-e', 'tls.handshake.version', # Client offered versions
                '-e', 'tls.handshake.ciphersuites', # Client offered ciphersuites
                '-e', 'tls.handshake.extensions_server_name', # SNI
                '-e', 'tls.handshake.extensions_ja3_hash', # JA3 from tshark
                '-E', 'separator=|', '-E', 'occurrence=f'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line:
                        parts = line.split('|')
                        if len(parts) >= 8: # Min fields for JA3 context
                            try:
                                conn = {
                                    'timestamp': float(parts[0]) if parts[0] else 0.0,
                                    'src_ip': parts[1], 'dst_ip': parts[2],
                                    'src_port': int(parts[3]) if parts[3] else 0,
                                    'dst_port': int(parts[4]) if parts[4] else 0,
                                    'tls_client_hello_version_offered': parts[5], # Could be a list
                                    'tls_client_ciphersuites_offered': parts[6], # Comma-separated list of IDs
                                    'server_name_indication': parts[7],
                                    'ja3_hash_tshark': parts[8] if len(parts) > 8 and parts[8] else None,
                                    # 'certificate': {} // Full cert parsing needs more specific tshark fields or post-processing
                                }
                                # Simplified JA3 if tshark doesn't provide it (less accurate)
                                if not conn['ja3_hash_tshark'] and conn['tls_client_hello_version_offered'] and conn['tls_client_ciphersuites_offered']:
                                     conn['ja3_hash_calculated_simple'] = self._generate_ja3_fingerprint(conn) # Use our simple one

                                tls_connections.append(conn)
                            except (ValueError, IndexError) as e_parse:
                                logger.debug(f"tshark: Error parsing TLS line '{line}': {e_parse}")
                                continue
            else:
                logger.warning(f"tshark TLS command failed: {result.stderr}")

        except subprocess.TimeoutExpired:
            logger.warning("tshark TLS extraction timed out.")
        except Exception as e:
            logger.warning(f"tshark TLS extraction error: {e}")
        
        return tls_connections

    def _extract_tls_with_scapy_basic(self, pcap_path:Path) -> List[Dict[str,Any]]:
        """Very basic TLS info extraction with Scapy if tshark is unavailable."""
        tls_connections = []
        if not SCAPY_AVAILABLE or not TLS:
            return tls_connections
        try:
            packets = scapy.rdpcap(str(pcap_path))
            for packet in packets:
                if packet.haslayer(TLS):
                    # This is very high level, Scapy's TLS layer parsing can be complex
                    # and might not easily give JA3 components or full cert details without significant effort.
                    try:
                        tls_layer = packet[TLS]
                        # Attempt to identify ClientHello or ServerHello for version/cipher info
                        # This is a simplification; real JA3 needs specific fields from ClientHello.
                        # Scapy's TLS dissection varies by version and complexity.
                        
                        # Example: try to get record version (might not be handshake version)
                        record_version_num = getattr(tls_layer, 'version', None)
                        record_version_str = ""
                        if record_version_num:
                            # TLS versions in Scapy might be like 0x0303 for TLS 1.2
                            if record_version_num == 0x0301: record_version_str = "TLS 1.0"
                            elif record_version_num == 0x0302: record_version_str = "TLS 1.1"
                            elif record_version_num == 0x0303: record_version_str = "TLS 1.2"
                            elif record_version_num == 0x0304: record_version_str = "TLS 1.3"
                            else: record_version_str = f"Unknown (0x{record_version_num:04x})"
                        
                        # Cipher suite is typically negotiated, found in ServerHello or derived from ClientHello options
                        # This is a placeholder, actual cipher suite extraction is more involved.
                        cipher_suite_val = "Unknown"
                        if hasattr(tls_layer, 'cipher'): # If a general cipher field exists
                            cipher_suite_val = str(tls_layer.cipher)
                        elif hasattr(tls_layer, 'msg') and tls_layer.msg:
                            # Look for ClientHello or ServerHello messages within the record
                            for msg in tls_layer.msg:
                                if hasattr(msg, 'cipher') and msg.cipher: # e.g. in ServerHello
                                    cipher_suite_val = str(msg.cipher) 
                                    break
                                elif hasattr(msg, 'ciphers') and msg.ciphers: # e.g. in ClientHello (list)
                                    cipher_suite_val = str(msg.ciphers[0] if msg.ciphers else "ClientOfferedUnknown")
                                    break


                        connection = {
                            'timestamp': float(packet.time),
                            'src_ip': packet[scapy.IP].src if packet.haslayer(scapy.IP) else '',
                            'dst_ip': packet[scapy.IP].dst if packet.haslayer(scapy.IP) else '',
                            'src_port': packet[scapy.TCP].sport if packet.haslayer(scapy.TCP) else 0,
                            'dst_port': packet[scapy.TCP].dport if packet.haslayer(scapy.TCP) else 0,
                            'tls_version': record_version_str, # This is record layer version
                            'cipher_suite': cipher_suite_val, # Highly simplified
                            'server_name_indication': '', # SNI is harder to get reliably with basic Scapy
                            'certificate': {} # Certificate parsing is very complex with Scapy
                        }
                        connection['ja3_hash_calculated_simple'] = self._generate_ja3_fingerprint(connection)
                        tls_connections.append(connection)
                    except Exception as e_pkt:
                        logger.debug(f"Scapy: Error processing individual TLS packet: {e_pkt}")
                        continue
        except Exception as e:
            logger.warning(f"Scapy basic TLS extraction error: {e}")
        return tls_connections


    def _is_suspicious_certificate(self, cert_info: Dict[str, Any]) -> bool:
        """Check if certificate (details dict) is suspicious. Placeholder - needs real cert fields."""
        # This method expects cert_info to be populated by a more detailed TLS parsing step
        # (e.g. from tshark fields like tls.handshake.x509ce.* or a dedicated cert parsing library)
        # For now, this is a placeholder as _extract_tls_connections is simplified.
        
        if cert_info.get('self_signed', False): # Assuming a field 'self_signed' exists
            return True
        
        validity_days = cert_info.get('validity_days', 365) # Assuming 'validity_days'
        if validity_days < 30 and validity_days > 0: 
            return True
        
        subject_cn = cert_info.get('subject_common_name', '').lower()
        issuer_cn = cert_info.get('issuer_common_name', '').lower()

        if subject_cn == issuer_cn and 'example' not in subject_cn : # Simplified self-signed check if subject CN = issuer CN
             # Be careful, some CAs might have same CN for root and intermediate.
             # This is a weak indicator.
             pass # Could be suspicious, but needs more context.

        suspicious_keywords = ['test', 'temp', 'localhost', 'example', 'snakeoil', 'default']
        if any(keyword in subject_cn for keyword in suspicious_keywords):
            return True
        if any(keyword in issuer_cn for keyword in suspicious_keywords):
            return True
            
        return False

    def _get_cert_suspicion_reasons(self, cert_info: Dict[str, Any]) -> List[str]:
        """Get reasons why certificate is suspicious. Placeholder."""
        reasons = []
        if cert_info.get('self_signed', False):
            reasons.append('self_signed_flag_true')
        
        validity_days = cert_info.get('validity_days', 365)
        if validity_days < 30 and validity_days > 0:
            reasons.append(f'short_validity_period_days:{validity_days}')
        
        subject_cn = cert_info.get('subject_common_name', '').lower()
        issuer_cn = cert_info.get('issuer_common_name', '').lower()
        suspicious_keywords = ['test', 'temp', 'localhost', 'example', 'snakeoil', 'default']

        for keyword in suspicious_keywords:
            if keyword in subject_cn:
                reasons.append(f'suspicious_keyword_in_subject_cn:{keyword}')
                break 
        for keyword in suspicious_keywords:
            if keyword in issuer_cn:
                reasons.append(f'suspicious_keyword_in_issuer_cn:{keyword}')
                break
        
        if not reasons and self._is_suspicious_certificate(cert_info): # Generic if other checks pass
            reasons.append('other_unspecified_suspicious_characteristics')
            
        return reasons

    def _generate_ja3_fingerprint(self, connection_info: Dict[str, Any]) -> Optional[str]:
        """
        Generate a simplified JA3-like fingerprint from TLS Client Hello info.
        A proper JA3 requires specific fields: SSLVersion,Cipher,SSLExtension,EllipticCurve,EllipticCurvePointFormat
        This is a much simpler version if those fields aren't readily available.
        """
        # If tshark provided JA3, prefer that.
        if connection_info.get('ja3_hash_tshark'):
            return connection_info['ja3_hash_tshark']

        try:
            # Use client offered versions and ciphersuites from connection_info
            # These would be populated by _extract_tls_connections (e.g. from tshark fields)
            tls_version_offered = str(connection_info.get('tls_client_hello_version_offered', ''))
            ciphers_offered = str(connection_info.get('tls_client_ciphersuites_offered', ''))
            # SNI can also be part of a more extended fingerprint but not standard JA3
            # sni = str(connection_info.get('server_name_indication', ''))
            
            if tls_version_offered and ciphers_offered:
                # Very simplified "fingerprint" string
                # Real JA3 is specific: SSLVersion,Cipher,SSLExtension,EllipticCurve,EllipticCurvePointFormat
                # Example: 771,4865-4866-4867-49195-49199-49196-49200-52393-52392-49171-49172-156-157-47-53,0-23-65281-10-11-35-16-5-13-18-51-45-43-27-17513,29-23-24,0
                # Our simplified version:
                ja3_string_parts = [
                    tls_version_offered.replace(',', '-'), # Normalize lists if they are comma separated
                    ciphers_offered.replace(',', '-')
                ]
                # Add more components if available and make sense for a simple hash
                # e.g. number of extensions, specific common extensions present.
                # For now, keeping it very basic.
                
                ja3_string = ",".join(ja3_string_parts)
                return hashlib.md5(ja3_string.encode()).hexdigest()
        except Exception as e:
            logger.debug(f"Error generating simplified JA3 hash: {e}")
            pass
        return None

    def _analyze_encrypted_flows(self, pcap_path: Path) -> Dict[str, Any]:
        """Analyze encrypted traffic flows (placeholder)"""
        flow_analysis = {
            'total_encrypted_flows': 0,
            'avg_flow_duration_seconds': 0.0,
            'avg_bytes_per_flow': 0.0,
            'flow_patterns_summary': [], # e.g. short bursts, long continuous
            'timing_analysis_summary': {}
        }
        logger.info("Placeholder: Encrypted flow analysis not yet implemented.")
        # This would involve identifying flows (e.g., 5-tuples), calculating duration,
        # byte counts, packet counts, inter-arrival times for encrypted protocols like TLS/SSH.
        return flow_analysis

    def _detect_port_anomalies(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect port/protocol mismatches (placeholder)"""
        anomalies = []
        logger.info("Placeholder: Port/protocol mismatch detection not yet implemented.")
        # Example: HTTP traffic on a non-standard port, or SSH on port 80.
        # This requires deeper packet inspection to identify the actual protocol vs the port.
        return anomalies

    def _detect_protocol_tunneling(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect protocol tunneling (other than DNS/ICMP - placeholder)"""
        tunneling_anomalies = []
        logger.info("Placeholder: Generic protocol tunneling detection not yet implemented.")
        # E.g., SSH over HTTP, other protocols within unexpected carriers.
        return tunneling_anomalies

    def _detect_packet_size_anomalies(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect unusual packet sizes across all traffic"""
        size_anomalies = []
        if not SCAPY_AVAILABLE: # Or other library if preferred for this
            logger.warning("Scapy not available for packet size anomaly detection.")
            return size_anomalies
        try:
            packets = scapy.rdpcap(str(pcap_path))
            if not packets:
                return size_anomalies

            packet_sizes = [len(p) for p in packets]
            
            if len(packet_sizes) > 10: # Need enough packets for meaningful stats
                avg_size = statistics.mean(packet_sizes)
                std_dev = statistics.stdev(packet_sizes) if len(packet_sizes) > 1 else 0.0
                
                # Look for packets significantly larger or smaller than average
                # These thresholds are arbitrary and need tuning.
                upper_threshold = avg_size + 3 * std_dev
                lower_threshold = max(0, avg_size - 2 * std_dev) # Avoid negative

                unusually_large_packets = [s for s in packet_sizes if s > upper_threshold and s > 1400] # e.g. > MTU
                unusually_small_packets = [s for s in packet_sizes if s < lower_threshold and s < 40] # e.g. very small control-like packets

                if len(unusually_large_packets) > len(packets) * 0.05: # If >5% are unusually large
                    size_anomalies.append({
                        'type': 'unusual_large_packet_sizes',
                        'count': len(unusually_large_packets),
                        'avg_observed_large_size': statistics.mean(unusually_large_packets) if unusually_large_packets else 0,
                        'description': f"{len(unusually_large_packets)} packets significantly larger than average.",
                        'severity_score': 0.3
                    })
                if len(unusually_small_packets) > len(packets) * 0.1: # If >10% are unusually small
                     size_anomalies.append({
                        'type': 'unusual_small_packet_sizes',
                        'count': len(unusually_small_packets),
                        'avg_observed_small_size': statistics.mean(unusually_small_packets) if unusually_small_packets else 0,
                        'description': f"{len(unusually_small_packets)} packets significantly smaller than average.",
                        'severity_score': 0.2
                    })
        except Exception as e:
            logger.warning(f"Packet size anomaly detection error: {e}", exc_info=True)
        return size_anomalies

    def _detect_timing_anomalies(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect timing-based anomalies (placeholder)"""
        timing_anomalies = []
        logger.info("Placeholder: Timing anomaly detection not yet implemented.")
        # E.g., perfectly regular inter-packet delays for non-beaconing traffic,
        # or sudden bursts of activity.
        return timing_anomalies

    def _detect_covert_channels(self, pcap_path: Path) -> List[Dict[str, Any]]:
        """Detect covert channels in network traffic (placeholder)"""
        covert_channels = []
        logger.info("Placeholder: Covert channel detection not yet implemented.")
        # E.g., data hidden in ICMP fields, TCP sequence numbers, DNS TXT records (partially covered by DNS tunneling).
        return covert_channels
    
    # --- Traffic Summary Generation Helpers ---
    def _generate_summary_with_scapy(self, pcap_path: Path) -> Dict[str, Any]:
        """Generate traffic summary using Scapy"""
        summary = {
            'total_packets': 0, 'total_bytes': 0, 'duration_seconds': 0.0,
            'start_time_epoch': None, 'end_time_epoch': None,
            'protocols': Counter(), 'top_source_ips': Counter(), 'top_dest_ips': Counter(),
            'top_source_ports': Counter(), 'top_dest_ports': Counter(),
            'packet_sizes': [], 'unique_ips': set(), 'internal_ips': set(), 'external_ips': set()
        }
        if not SCAPY_AVAILABLE:
            logger.warning("Scapy not available for summary generation.")
            return summary
        try:
            # For very large PCAPs, consider pcapiter or process in chunks
            packets = scapy.rdpcap(str(pcap_path))
            summary['total_packets'] = len(packets)
            if not packets: return summary # No packets to summarize

            timestamps = []
            for packet in packets:
                summary['total_bytes'] += len(packet)
                summary['packet_sizes'].append(len(packet))
                timestamps.append(float(packet.time))
                
                src_ip, dst_ip = None, None
                if packet.haslayer(scapy.IP):
                    src_ip = packet[scapy.IP].src
                    dst_ip = packet[scapy.IP].dst
                elif packet.haslayer(scapy.IPv6): # Add basic IPv6 support
                    src_ip = packet[scapy.IPv6].src
                    dst_ip = packet[scapy.IPv6].dst
                
                if src_ip and dst_ip:
                    summary['top_source_ips'][src_ip] += 1
                    summary['top_dest_ips'][dst_ip] += 1
                    summary['unique_ips'].add(src_ip)
                    summary['unique_ips'].add(dst_ip)
                    
                    if self._is_private_ip(src_ip): summary['internal_ips'].add(src_ip)
                    else: summary['external_ips'].add(src_ip)
                    if self._is_private_ip(dst_ip): summary['internal_ips'].add(dst_ip)
                    else: summary['external_ips'].add(dst_ip)

                # Count protocols and ports
                if packet.haslayer(scapy.TCP):
                    summary['protocols']['TCP'] += 1
                    summary['top_source_ports'][str(packet[scapy.TCP].sport)] += 1 # Ports as strings for Counter keys
                    summary['top_dest_ports'][str(packet[scapy.TCP].dport)] += 1
                elif packet.haslayer(scapy.UDP):
                    summary['protocols']['UDP'] += 1
                    summary['top_source_ports'][str(packet[scapy.UDP].sport)] += 1
                    summary['top_dest_ports'][str(packet[scapy.UDP].dport)] += 1
                elif packet.haslayer(scapy.ICMP):
                    summary['protocols']['ICMP'] += 1
                elif packet.haslayer(scapy.ARP):
                     summary['protocols']['ARP'] +=1
                else: # Other layer 3/4 protocols
                    # Try to get protocol name if IP layer exists
                    if packet.haslayer(scapy.IP) and hasattr(scapy.IP,'proto'):
                        proto_num = packet[scapy.IP].proto
                        # Common protocol numbers: 1 (ICMP), 6 (TCP), 17 (UDP)
                        # Others can be looked up or kept as numbers
                        summary['protocols'][f'IP_Proto_{proto_num}'] +=1
                    else:
                        summary['protocols']['Other_L2/L3'] +=1


            if timestamps:
                summary['start_time_epoch'] = min(timestamps)
                summary['end_time_epoch'] = max(timestamps)
                summary['duration_seconds'] = summary['end_time_epoch'] - summary['start_time_epoch']
                
        except Exception as e:
            logger.warning(f"Scapy summary generation error: {e}", exc_info=True)
        return summary

    def _generate_summary_with_dpkt(self, pcap_path: Path) -> Dict[str, Any]:
        """Generate traffic summary using dpkt"""
        summary = {
            'total_packets': 0, 'total_bytes': 0, 'duration_seconds': 0.0,
            'start_time_epoch': None, 'end_time_epoch': None,
            'protocols': Counter(), 'top_source_ips': Counter(), 'top_dest_ips': Counter(),
            'top_source_ports': Counter(), 'top_dest_ports': Counter(),
            'packet_sizes': [], 'unique_ips': set(), 'internal_ips': set(), 'external_ips': set()
        }
        if not DPKT_AVAILABLE:
            logger.warning("dpkt not available for summary generation.")
            return summary
        try:
            with open(pcap_path, 'rb') as f:
                pcap_reader = dpkt.pcap.Reader(f)
                timestamps = []
                
                for timestamp, buf in pcap_reader:
                    summary['total_packets'] += 1
                    summary['total_bytes'] += len(buf)
                    summary['packet_sizes'].append(len(buf))
                    timestamps.append(timestamp)
                    
                    try:
                        eth = dpkt.ethernet.Ethernet(buf)
                        # Handle IP (v4 and v6 if dpkt supports it well enough for this)
                        if isinstance(eth.data, dpkt.ip.IP): # IPv4
                            ip = eth.data
                            src_ip = self._ip_to_str(ip.src)
                            dst_ip = self._ip_to_str(ip.dst)

                            summary['top_source_ips'][src_ip] += 1
                            summary['top_dest_ips'][dst_ip] += 1
                            summary['unique_ips'].add(src_ip)
                            summary['unique_ips'].add(dst_ip)
                            
                            if self._is_private_ip(src_ip): summary['internal_ips'].add(src_ip)
                            else: summary['external_ips'].add(src_ip)
                            if self._is_private_ip(dst_ip): summary['internal_ips'].add(dst_ip)
                            else: summary['external_ips'].add(dst_ip)

                            if isinstance(ip.data, dpkt.tcp.TCP):
                                summary['protocols']['TCP'] += 1
                                summary['top_source_ports'][str(ip.data.sport)] += 1
                                summary['top_dest_ports'][str(ip.data.dport)] += 1
                            elif isinstance(ip.data, dpkt.udp.UDP):
                                summary['protocols']['UDP'] += 1
                                summary['top_source_ports'][str(ip.data.sport)] += 1
                                summary['top_dest_ports'][str(ip.data.dport)] += 1
                            elif isinstance(ip.data, dpkt.icmp.ICMP):
                                summary['protocols']['ICMP'] += 1
                            else:
                                summary['protocols'][f'IP_Proto_{ip.p}'] += 1
                        elif isinstance(eth.data, dpkt.arp.ARP): # ARP
                            summary['protocols']['ARP'] +=1
                        # Add IPv6 basic handling if dpkt.ip6.IP6 exists and is used
                        # elif isinstance(eth.data, dpkt.ip6.IP6):
                        #     summary['protocols']['IPv6'] +=1 # Generic count
                        else:
                            summary['protocols']['Other_L2/L3'] +=1
                    except Exception: # Catch errors parsing individual packets
                        logger.debug("dpkt: Error parsing packet layers for summary.")
                        continue
                
                if timestamps:
                    summary['start_time_epoch'] = min(timestamps)
                    summary['end_time_epoch'] = max(timestamps)
                    summary['duration_seconds'] = summary['end_time_epoch'] - summary['start_time_epoch']
        except Exception as e:
            logger.warning(f"dpkt summary generation error: {e}", exc_info=True)
        return summary

    def _generate_summary_with_tshark(self, pcap_path: Path) -> Dict[str, Any]:
        """Generate basic traffic summary using tshark's io,stat. More detailed summary needs more complex tshark calls."""
        summary = {
            'total_packets': 0, 'total_bytes': 0, 'duration_seconds': 0.0,
            'start_time_epoch': None, 'end_time_epoch': None, # These are harder with just io,stat
            'protocols': Counter(), 'unique_ips': set() # Basic fields from simple tshark
        }
        try:
            subprocess.run(['tshark', '-v'], capture_output=True, check=False)
        except FileNotFoundError:
            logger.warning("tshark command not found. Cannot generate summary with tshark.")
            return summary
        try:
            # Get overall stats (packets, bytes, duration)
            # tshark -r <file> -qz io,stat,0,"COUNT(frame)frame","SUM(frame.len)frame.len","MIN(frame.time_epoch)frame.time_epoch","MAX(frame.time_epoch)frame.time_epoch"
            # This is complex to parse reliably. A simpler io,stat gives total packets/bytes.
            cmd_stat = ['tshark', '-r', str(pcap_path), '-q', '-z', 'io,stat,0']
            result_stat = subprocess.run(cmd_stat, capture_output=True, text=True, timeout=120)
            
            if result_stat.returncode == 0:
                # Example io,stat output:
                # ===================================================================
                # IO Statistics
                # Duration: 5.123 s
                # Interval: 5.123 s
                # Frames: 100
                # Bytes: 12345
                # ===================================================================
                for line in result_stat.stdout.split('\n'):
                    if 'Frames:' in line:
                        try: summary['total_packets'] = int(line.split(':')[1].strip())
                        except: pass
                    if 'Bytes:' in line:
                        try: summary['total_bytes'] = int(line.split(':')[1].strip())
                        except: pass
                    if 'Duration:' in line: # tshark io,stat provides duration directly
                        try: summary['duration_seconds'] = float(line.split(':')[1].strip().split(' ')[0])
                        except: pass
            else:
                logger.warning(f"tshark io,stat command failed: {result_stat.stderr}")

            # Get protocol hierarchy for protocol distribution (can be slow)
            # cmd_protos = ['tshark', '-r', str(pcap_path), '-q', '-z', 'io,phs']
            # result_protos = subprocess.run(cmd_protos, capture_output=True, text=True, timeout=120)
            # if result_protos.returncode == 0:
            #    # Parse protocol hierarchy output (complex)
            #    pass # Placeholder for parsing protocol stats

        except subprocess.TimeoutExpired:
            logger.warning("tshark summary generation timed out.")
        except Exception as e:
            logger.warning(f"tshark summary generation error: {e}", exc_info=True)
        return summary

    def _generate_threat_indicators(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate high-level threat indicators from analysis results"""
        indicators = []
        
        # C2 Communications
        c2_comms = analysis_results.get('c2_communications', [])
        if c2_comms:
            high_confidence_c2 = [c for c in c2_comms if c.get('confidence_score', 0) > 0.6] # Adjusted threshold
            if high_confidence_c2:
                indicators.append({
                    'type': 'command_and_control',
                    'severity': 'critical',
                    'count': len(high_confidence_c2),
                    'description': f"Detected {len(high_confidence_c2)} high-confidence C2 communications. Destinations: {[c.get('destination') for c in high_confidence_c2[:3]]}" # Sample destinations
                })
        
        # DGA Domains
        dga_domains = analysis_results.get('dga_domains', [])
        if dga_domains:
            high_score_dga = [d for d in dga_domains if d.get('dga_score', 0) > 0.7]
            if high_score_dga:
                indicators.append({
                    'type': 'domain_generation_algorithm',
                    'severity': 'high',
                    'count': len(high_score_dga),
                    'description': f"Detected {len(high_score_dga)} DGA-like domains with high scores. Examples: {[d.get('domain') for d in high_score_dga[:3]]}"
                })
        
        # Data Exfiltration
        exfiltration = analysis_results.get('data_exfiltration', [])
        if exfiltration:
            # Sum DNS tunneling and HTTP exfil for a combined indicator
            dns_exfil_count = sum(1 for e in exfiltration if e.get('type') == 'dns_tunneling_suspicion' and e.get('confidence',0) > 0.6)
            http_exfil_count = sum(1 for e in exfiltration if 'http' in e.get('type','').lower() and e.get('confidence',0) > 0.5)
            total_exfil_detected = dns_exfil_count + http_exfil_count

            if total_exfil_detected > 0:
                descriptions = []
                if dns_exfil_count > 0: descriptions.append(f"{dns_exfil_count} DNS tunneling attempts")
                if http_exfil_count > 0: descriptions.append(f"{http_exfil_count} HTTP exfiltration attempts")
                
                indicators.append({
                    'type': 'data_exfiltration_suspected',
                    'severity': 'critical' if dns_exfil_count > 0 else 'high', # DNS tunneling often more critical
                    'count': total_exfil_detected,
                    'description': f"Potential data exfiltration: {'; '.join(descriptions)}."
                })
        
        # Suspicious DNS Activity (beyond DGA)
        dns_analysis = analysis_results.get('dns_analysis', {})
        suspicious_dns_queries = dns_analysis.get('suspicious_queries', [])
        # Filter out DGA already counted, count others (e.g. known malicious, high entropy not DGA)
        non_dga_suspicious_dns = [q for q in suspicious_dns_queries if not any(d['domain'] == q.get('domain') for d in dga_domains)]
        if len(non_dga_suspicious_dns) > 5: 
            indicators.append({
                'type': 'suspicious_dns_activity_other',
                'severity': 'medium',
                'count': len(non_dga_suspicious_dns),
                'description': f"Detected {len(non_dga_suspicious_dns)} other suspicious DNS queries (e.g., to known malicious, high entropy). Examples: {[q.get('domain') for q in non_dga_suspicious_dns[:3]]}"
            })
        
        # Suspicious Encrypted Traffic
        encrypted_analysis = analysis_results.get('encrypted_traffic', {})
        suspicious_certs = encrypted_analysis.get('suspicious_certificates', [])
        if suspicious_certs:
             indicators.append({
                'type': 'suspicious_tls_certificates',
                'severity': 'medium',
                'count': len(suspicious_certs),
                'description': f"Detected {len(suspicious_certs)} suspicious TLS certificates. Destinations: {[c.get('destination_ip') or c.get('server_name_indication') for c in suspicious_certs[:3]]}"
            })
        # Add JA3 related indicators if specific known malicious JA3s are checked (not done here)

        # Protocol Anomalies (generic)
        anomalies = analysis_results.get('protocol_anomalies', [])
        if anomalies:
            high_severity_anomalies = [a for a in anomalies if a.get('severity_score', 0) > 0.5]
            if high_severity_anomalies:
                indicators.append({
                    'type': 'protocol_anomalies_detected',
                    'severity': 'medium',
                    'count': len(high_severity_anomalies),
                    'description': f"Detected {len(high_severity_anomalies)} protocol anomalies. Types: {[a.get('type') for a in high_severity_anomalies[:3]]}"
                })
        return indicators
    
    # --- Utility methods ---
    def _ip_to_str(self, ip_bytes: bytes) -> str:
        """Convert IP bytes (from dpkt) to string"""
        try:
            return ipaddress.ip_address(ip_bytes).compressed # Use compressed for IPv6
        except ValueError: # Handle cases where it might not be a standard IP format or length
             # Try common IPv4 length
            if len(ip_bytes) == 4:
                return ".".join(map(str, ip_bytes))
            logger.debug(f"Could not convert bytes to IP string: {ip_bytes!r}")
            return ''
    
    def _is_private_ip(self, ip_str: str) -> bool:
        """Check if IP address string is private"""
        if not ip_str: return False
        try:
            ip_obj = ipaddress.ip_address(ip_str)
            return ip_obj.is_private
        except ValueError: # Not a valid IP string
            return False
    
    def _times_correlate(self, time1_epoch: float, time2_epoch: float, window_seconds: int) -> bool:
        """Check if two epoch timestamps correlate within a time window"""
        return abs(time1_epoch - time2_epoch) <= window_seconds
    
    def _is_connection_related_to_process(self, connection_details: Dict[str, Any], process_details: Dict[str, Any], behavioral_results: Dict) -> bool:
        """Check if network connection is related to a process (Placeholder for correlation logic)"""
        # Placeholder: requires detailed behavioral data (process start times, PIDs, netstat-like output)
        # and matching with connection timestamps, local ports, etc.
        # conn_time = connection_details.get('timestamp')
        # proc_start_time = process_details.get('start_time') # Assuming this exists in behavioral
        # if conn_time and proc_start_time and conn_time >= proc_start_time:
        #     # Further checks: local port used by process, etc.
        #     return True 
        return False
    
    def _calculate_correlation_confidence(self, connection_details: Dict[str, Any], process_details: Dict[str, Any]) -> float:
        """Calculate confidence score for process-network correlation (Placeholder)"""
        # Placeholder: Score based on strength of evidence (e.g., exact port match, timing proximity)
        return 0.5 # Default placeholder confidence
    
    def _generate_timeline_correlation(self, network_results: Dict, behavioral_results: Dict) -> Dict[str, Any]:
        """Generate timeline correlation between network and behavioral events (Placeholder)"""
        logger.info("Placeholder: Timeline correlation generation not yet implemented.")
        return {
            'correlated_events': [], # List of (timestamp, event_type, description, source)
            'overall_confidence': 0.0
        }
    
    def _generate_attribution_analysis(self, network_results: Dict, behavioral_results: Dict) -> Dict[str, Any]:
        """Generate attribution analysis linking network activity to malware components (Placeholder)"""
        logger.info("Placeholder: Attribution analysis generation not yet implemented.")
        return {
            'attributed_ioc_sets': [], # e.g. { 'malware_family_guess': 'X', 'related_ips': [], 'related_domains': [] }
            'confidence_scores': {}
        }

if __name__ == '__main__':
    # Basic configuration for logging if run as a script
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger.info("Network Analysis Module - Standalone Test Mode")

    # Example usage (requires a pcap file)
    # Replace 'test.pcap' with an actual PCAP file path
    pcap_file_path = Path('test.pcap') 

    if not pcap_file_path.exists():
        logger.error(f"Test PCAP file not found: {pcap_file_path}. Please create a dummy or real pcap file for testing.")
        # Create a dummy pcap for basic testing if it doesn't exist
        if SCAPY_AVAILABLE:
            try:
                logger.info(f"Creating a dummy pcap file: {pcap_file_path}")
                # Simple ICMP packet
                dummy_packet = scapy.Ether()/scapy.IP(dst="8.8.8.8")/scapy.ICMP()
                scapy.wrpcap(str(pcap_file_path), [dummy_packet for _ in range(5)]) # Write 5 dummy packets
                logger.info(f"Dummy pcap created with 5 ICMP packets.")
            except Exception as e_dummy:
                 logger.error(f"Failed to create dummy pcap: {e_dummy}")
                 exit(1)
        else:
            logger.error("Scapy not available to create a dummy pcap. Please provide a test.pcap file.")
            exit(1)


    analyzer = NetworkAnalyzer()
    
    try:
        results = analyzer.analyze_pcap_file(pcap_file_path)
        logger.info("\n--- Analysis Results ---")
        
        # Pretty print parts of the results
        print(json.dumps({
            'pcap_info': results.get('pcap_info'),
            'traffic_summary_short': {
                'total_packets': results.get('traffic_summary',{}).get('total_packets'),
                'total_bytes': results.get('traffic_summary',{}).get('total_bytes'),
                'protocols': results.get('traffic_summary',{}).get('protocols')
            },
            'dns_queries_count': results.get('dns_analysis',{}).get('total_queries'),
            'suspicious_dns_count': len(results.get('dns_analysis',{}).get('suspicious_queries',[])),
            'dga_detected_count': len(results.get('dga_domains',[])),
            'http_requests_count': results.get('http_analysis',{}).get('total_requests'),
            'suspicious_http_count': len(results.get('http_analysis',{}).get('suspicious_requests',[])),
            'c2_detected_count': len(results.get('c2_communications',[])),
            'data_exfil_attempts_count': len(results.get('data_exfiltration',[])),
            'tls_connections_count': len(results.get('encrypted_traffic',{}).get('tls_connections',[])),
            'suspicious_certs_count': len(results.get('encrypted_traffic',{}).get('suspicious_certificates',[])),
            'protocol_anomalies_count': len(results.get('protocol_anomalies',[])),
            'threat_indicators': results.get('threat_indicators'),
            'network_iocs_counts': {k: len(v) for k,v in results.get('network_iocs',{}).items()}
        }, indent=2))

        if results.get('errors'):
            logger.warning("\n--- Analysis Errors ---")
            for err in results['errors']:
                logger.warning(err)

    except FileNotFoundError as e:
        logger.error(f"Error during analysis: {e}")
    except Exception as e:
        logger.error(f"An unexpected error occurred during analysis: {e}", exc_info=True)

